{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Welcome to the documentation for k8s@spiti which aims to support running Kubernetes at home. Features \u00b6 Provide Helm charts to help define, install, and upgrade even the most complex applications in Kubernetes. Provide container images that support Semantic Versioning , multiple architectures, and designed to run in a Kubernetes cluster. Provide community support for people wanting to learn and deploy Kubernetes in their homes. How do find people running Kubernetes at home? \u00b6 Repository topics \u00b6 Have a look at people running Kubernetes at home on GitHub and on GitLab . License \u00b6 This project is licensed under the terms of the Apache 2.0 License license.","title":"Home"},{"location":"#welcome","text":"Welcome to the documentation for k8s@spiti which aims to support running Kubernetes at home.","title":"Welcome"},{"location":"#features","text":"Provide Helm charts to help define, install, and upgrade even the most complex applications in Kubernetes. Provide container images that support Semantic Versioning , multiple architectures, and designed to run in a Kubernetes cluster. Provide community support for people wanting to learn and deploy Kubernetes in their homes.","title":"Features"},{"location":"#how-do-find-people-running-kubernetes-at-home","text":"","title":"How do find people running Kubernetes at home?"},{"location":"#repository-topics","text":"Have a look at people running Kubernetes at home on GitHub and on GitLab .","title":"Repository topics"},{"location":"#license","text":"This project is licensed under the terms of the Apache 2.0 License license.","title":"License"},{"location":"guides/dyndns/","text":"Introduction \u00b6 This is a guide on how to use native k8s CronJob s to sync your WAN IP to a dns provider. Creating a ConfigMap \u00b6 Let's get started by creating a configmap with the script to update to your DNS provider. Note In this example we will be using DigitalOcean This can work with any provider as long as your script works A quick search yielded this gist . Let's go ahead and update it a bit to work in our CronJob while putting it in a configmap . # configmap.yaml --- apiVersion : v1 kind : ConfigMap metadata : name : dyndns-updater namespace : default labels : app.kubernetes.io/name : dyndns-updater app.kubernetes.io/instance : dyndns-updater data : dyndns-updater.sh : | #!/bin/sh # Get your WAN IP IP=$(curl -s checkip.dyndns.org | grep -Eo '[0-9\\.]+') # Get the Record ID associated with the $DOMAIN RECORD_ID=$(curl -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${TOKEN}\" \"https://api.digitalocean.com/v2/domains/${DOMAIN}/records\") # Send request to update DNS Record curl -s -X PUT \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -d \"{\\\"data\\\":\\\"${IP}\\\"}\" \\ \"https://api.digitalocean.com/v2/domains/${DOMAIN}/records/${RECORD_ID}\" Creating a Secret \u00b6 Next step is to create a secret, this will require a $TOKEN passed thru to the CronJob . # secret.yaml --- apiVersion : v1 kind : Secret metadata : name : do-token namespace : default type : Opaque data : # Your domain name in base64 DOMAIN : fWataR3= # You API token in base64 # Get token from https://cloud.digitalocean.com/settings/applications TOKEN : YWRtaW4= Creating the CronJob \u00b6 Last step is to put it altogether is creating the CronJob . This will run every hour, on the hour. Tweak the cron formula as needed. # cronjob.yaml --- apiVersion : batch/v1beta1 kind : CronJob metadata : namespace : default name : dyndns-updater spec : schedule : \"0 * * * *\" failedJobsHistoryLimit : 1 successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : spec : template : spec : restartPolicy : Never containers : - name : dyndns-updater image : curlimages/curl:7.75.0 imagePullPolicy : IfNotPresent envFrom : - secretRef : name : do-token command : - \"/bin/sh\" - \"/app/dyndns-updater.sh\" volumeMounts : - name : dyndns-updater mountPath : /app/dyndns-updater.sh subPath : dyndns-updater.sh readOnly : true volumes : - name : dyndns-updater projected : defaultMode : 0775 sources : - configMap : name : dyndns-updater items : - key : dyndns-updater.sh path : dyndns-updater.sh","title":"DynDNS with a CronJob"},{"location":"guides/dyndns/#introduction","text":"This is a guide on how to use native k8s CronJob s to sync your WAN IP to a dns provider.","title":"Introduction"},{"location":"guides/dyndns/#creating-a-configmap","text":"Let's get started by creating a configmap with the script to update to your DNS provider. Note In this example we will be using DigitalOcean This can work with any provider as long as your script works A quick search yielded this gist . Let's go ahead and update it a bit to work in our CronJob while putting it in a configmap . # configmap.yaml --- apiVersion : v1 kind : ConfigMap metadata : name : dyndns-updater namespace : default labels : app.kubernetes.io/name : dyndns-updater app.kubernetes.io/instance : dyndns-updater data : dyndns-updater.sh : | #!/bin/sh # Get your WAN IP IP=$(curl -s checkip.dyndns.org | grep -Eo '[0-9\\.]+') # Get the Record ID associated with the $DOMAIN RECORD_ID=$(curl -X GET -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${TOKEN}\" \"https://api.digitalocean.com/v2/domains/${DOMAIN}/records\") # Send request to update DNS Record curl -s -X PUT \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer $TOKEN\" \\ -d \"{\\\"data\\\":\\\"${IP}\\\"}\" \\ \"https://api.digitalocean.com/v2/domains/${DOMAIN}/records/${RECORD_ID}\"","title":"Creating a ConfigMap"},{"location":"guides/dyndns/#creating-a-secret","text":"Next step is to create a secret, this will require a $TOKEN passed thru to the CronJob . # secret.yaml --- apiVersion : v1 kind : Secret metadata : name : do-token namespace : default type : Opaque data : # Your domain name in base64 DOMAIN : fWataR3= # You API token in base64 # Get token from https://cloud.digitalocean.com/settings/applications TOKEN : YWRtaW4=","title":"Creating a Secret"},{"location":"guides/dyndns/#creating-the-cronjob","text":"Last step is to put it altogether is creating the CronJob . This will run every hour, on the hour. Tweak the cron formula as needed. # cronjob.yaml --- apiVersion : batch/v1beta1 kind : CronJob metadata : namespace : default name : dyndns-updater spec : schedule : \"0 * * * *\" failedJobsHistoryLimit : 1 successfulJobsHistoryLimit : 3 concurrencyPolicy : Forbid jobTemplate : spec : template : spec : restartPolicy : Never containers : - name : dyndns-updater image : curlimages/curl:7.75.0 imagePullPolicy : IfNotPresent envFrom : - secretRef : name : do-token command : - \"/bin/sh\" - \"/app/dyndns-updater.sh\" volumeMounts : - name : dyndns-updater mountPath : /app/dyndns-updater.sh subPath : dyndns-updater.sh readOnly : true volumes : - name : dyndns-updater projected : defaultMode : 0775 sources : - configMap : name : dyndns-updater items : - key : dyndns-updater.sh path : dyndns-updater.sh","title":"Creating the CronJob"},{"location":"guides/pod-gateway/","text":"Introduction \u00b6 This is a guide on how to send all traffic from a group of pods to a gateway pod. The gateway pod will then typically use a VPN to route the traffic further. Requirements \u00b6 one or more namespaces where you deploy pods to be routed another namespace to deploy the gateway pod to. It is critical to deploy the gateway to a different namespace as the routed pods. (optional) VPN client settings (credentials, hostname, etc) Deploying the pod gateway \u00b6 Namespace \u00b6 You need a namespace with the label routed-gateway set to true. In this tutorial the namespace will be called vpn : # namespace.yaml --- apiVersion : v1 kind : Namespace metadata : name : vpn labels : routed-gateway : \"true\" pod-gateway Helm release \u00b6 You need to deploy the pod-gateway helm chart. Assuming you use GitOps to deploy the routed pods, you might use the following template to deploy the gateway pod: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway namespace : default labels : spec : interval : 5m chart : spec : # renovate: registryUrl=https://k8s-at-spiti.com/charts/ chart : pod-gateway version : 2.0.0 sourceRef : kind : HelmRepository name : k8s-at-spiti-charts namespace : flux-system interval : 5m # See https://github.com/k8s-at-spiti/charts/blob/master/charts/pod-gateway/values.yaml values : routed_namespaces : - vpn The above should deploy a gateway and a gateway-hook: the gateway-hook has the task of modifying created PODs in the vpn namespace to be configured to use the VPN gateway. You will find more details in its git repository . the gateway provides a VXLAN tunnel, a DHCP server and a DNS server for client pods to connect to. Optionally it also runs the VPN client. You will find more details in its git repository . Test deployment \u00b6 Then you can deploy a test deployment to test it: # TestDeployment.yaml --- apiVersion : apps/v1 kind : Deployment metadata : name : terminal namespace : vpn labels : app : terminal spec : replicas : 1 selector : matchLabels : app : terminal template : metadata : labels : app : terminal spec : containers : - name : alpine image : alpine command : - /bin/sh - -c - while true; do sleep 600 & wait $!; done If you exec into it you should see the traffic being routed through the gateway: root@k3s1:~# kubectl exec -ti -n vpn deploy/terminal -- sh / # ip route default via 172 .16.0.1 dev vxlan0 10 .0.0.0/8 via 10 .0.2.1 dev eth0 10 .0.2.0/24 dev eth0 scope link src 10 .0.2.174 172 .16.0.0/24 dev vxlan0 scope link src 172 .16.0.133 / # ip addr 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3 : eth0@if228: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue state UP link/ether 6a:e8:0b:31:eb:3e brd ff:ff:ff:ff:ff:ff inet 10 .0.2.174/24 brd 10 .0.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::68e8:bff:fe31:eb3e/64 scope link valid_lft forever preferred_lft forever 4 : vxlan0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UNKNOWN qlen 1000 link/ether 06 :81:5d:8c:4a:15 brd ff:ff:ff:ff:ff:ff inet 172 .16.0.133/24 brd 172 .16.0.255 scope global vxlan0 valid_lft forever preferred_lft forever inet6 fe80::481:5dff:fe8c:4a15/64 scope link valid_lft forever preferred_lft forever / # cat /etc/resolv.conf nameserver 172 .16.0.1 / # ping -c1 example.com 64 bytes from 93 .184.216.34: seq = 0 ttl = 115 time = 36 .366 ms --- example.com ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 36 .366/36.366/36.366 ms The important part is that the default gateway and the DNS are set to 172.16.0.1 which is the default IP of the gateway POD in the vlxlan network. If this is the case then you are ready for the (optional) VPN setup. If the ping does not work and you are using Calico please check the Calico section bellow. Network Policy \u00b6 For additional precaution, you should deploy a network policy into each routed namespace to prevent traffic leaving the K8S cluster without passing through the pod gateway. This way, even in case of setup errors, you will not leak any traffic. # NetworkPolicy.yaml --- kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : vpn-namespace spec : podSelector : {} ingress : - from : # Only allow ingress from K8S - ipBlock : cidr : 10.0.0.0/8 egress : - to : # Only allow egress to K8S - ipBlock : cidr : 10.0.0.0/8 policyTypes : - Ingress - Egress Additional Configuration \u00b6 VPN \u00b6 You will need to enable the vpn sidecar in the helm release settings: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... addons : vpn : enabled : true # You might use `openvpn` or `wireguard` type : openvpn openvpn : # VPN settings stored in secret `vpnConfig`. The secret mus have a key # a key called `vpnConfigfile` with the openvpn/wireguard config files in them configFileSecret : openvpn livenessProbe : exec : # In the example bellow the VPN output is in Belgic (BE) - change appropiatly command : - sh - -c - if [ $(wget -q -O- https://ipinfo.io/country) == 'BE' ]; then exit 0; else exit $?; fi initialDelaySeconds : 30 periodSeconds : 60 failureThreshold : 1 networkPolicy : enabled : true egress : - to : - ipBlock : cidr : 0.0.0.0/0 ports : # VPN traffic port - change if your provider uses a different port - port : 443 protocol : UDP - to : # Allow traffic within K8S - change if your K8S cluster uses a different CIDR - ipBlock : cidr : 10.0.0.0/8 settings : # tun0 for openvpn, wg0 for wireguard VPN_INTERFACE : tun0 # Prevent non VPN traffic to leave the gateway VPN_BLOCK_OTHER_TRAFFIC : true # If VPN_BLOCK_OTHER_TRAFFIC is true, allow VPN traffic over this port VPN_TRAFFIC_PORT : 443 # Traffic to these IPs will be send through the K8S gateway # change if your K8S cluster or home network uses a different CIDR VPN_LOCAL_CIDRS : \"10.0.0.0/8 192.168.0.0/16\" Exposing routed pod ports from the gateway \u00b6 You can expose individual ports of routed PODs thought he pod gateway. This is specially useful if you need to expose PODs to the Internet through the VPN server. For example, you can expose the torrent port or a web server. You need to ensure the exposed routed pod has a static name. If you use the k8s-at-spiti charts, this is done by setting the hostname value: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : qbittorrent spec : ... values : hostname : torrent # needed for an static IP Then you can expose the port by extending the pod-gateway helm-release: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... # -- settings to expose ports, usually through a VPN provider. # NOTE : if you change it you will need to manually restart the gateway POD publicPorts : - hostname : torrent #hostname assigned to the pod IP : 10 # must be an integer between 2 and VXLAN_GATEWAY_FIRST_DYNAMIC_IP (20 by default) ports : - type : udp port : 18289 - type : tcp port : 18289 Calico \u00b6 Calico only configures a single default gateway and not dedicated rules for traffic within the K8S cluster. So you will need to add that explictly by extending the pod-gateway helm chart deployment: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... settings : # Route internal K8s and local home traffic in to the defaullt K8S gateway NOT_ROUTED_TO_GATEWAY_CIDRS : \"172.22.0.0/12 <local home network CIDR>\" VPN_LOCAL_CIDRS : \"172.22.0.0/12 <local home network CIDR>\" # Use a different VXLAN network segment that does not conflict with the above VXLAN_IP_NETWORK : \"192.168.242.0/24\" How to debug \u00b6 Connectivity \u00b6 From the gateway and routed pods check you can ping the following: K8S DNS IP (check an stardard port to find out) if this fails, then likelly you forgot setting NOT_ROUTED_TO_GATEWAY_CIDRS . Please read the Calico setup. from the routed pod, the gateway IP (172.16.0.1 by default) if this fails, there is problem with the VXLAN setup and more debugging is needed from the gateway pod, the routed pod if this fails, there is problem with the VXLAN setup and more debugging is needed an IP in your home network if this fails, then likelly you forgot setting VPN_LOCAL_CIDRS to include your K8S CIDR - the default 10.0.0.0/8 works in Flannel (k3s default) but not in Calico. a hostname in Internet without VPN: if this fails, you might have set the VPN_BLOCK_OTHER_TRAFFIC variable to true with VPN: if this fails, check the following: output of the wireguard VPN_INTERFACE - must be wg0 for wireguard VPN_TRAFFIC_PORT - the default, 443 works for some openvpn servers VPN_LOCAL_CIDRS - it must include the K8S and local home CIDRs a hostname if this fails, there is problem with the DNS resolution. More debugging is needed Routed Pod Fails to init \u00b6 If the test pod/terminal pod fails to get passed the init container, check the logs from the init container kubectl -n vpn logs terminal-xdfexampleaex-pvfpc gateway-init If you see + GATEWAY_IP=';; connection timed out; no servers could be reached' , try setting the NOT_ROUTED_TO_GATEWAY_CIDRS: with your cluster cidr and service cidrs. For example, if you're running k3s with the default flannel install , it will be NOT_ROUTED_TO_GATEWAY_CIDRS: \"10.42.0.0/16 10.43.0.0/16\" Collecting more debug \u00b6 Collect the following information from the routed and gateway pods: ip route ip addr cat /etc/resolv.conf ping -c1 example.com You can compare with the output of the deployment example above. If you are not able to tell what the problem is you might open an issue for chart pod-gateway . Please attach there the connectivity test results and the additional debug information.","title":"Routing traffic through a VPN pod"},{"location":"guides/pod-gateway/#introduction","text":"This is a guide on how to send all traffic from a group of pods to a gateway pod. The gateway pod will then typically use a VPN to route the traffic further.","title":"Introduction"},{"location":"guides/pod-gateway/#requirements","text":"one or more namespaces where you deploy pods to be routed another namespace to deploy the gateway pod to. It is critical to deploy the gateway to a different namespace as the routed pods. (optional) VPN client settings (credentials, hostname, etc)","title":"Requirements"},{"location":"guides/pod-gateway/#deploying-the-pod-gateway","text":"","title":"Deploying the pod gateway"},{"location":"guides/pod-gateway/#namespace","text":"You need a namespace with the label routed-gateway set to true. In this tutorial the namespace will be called vpn : # namespace.yaml --- apiVersion : v1 kind : Namespace metadata : name : vpn labels : routed-gateway : \"true\"","title":"Namespace"},{"location":"guides/pod-gateway/#pod-gateway-helm-release","text":"You need to deploy the pod-gateway helm chart. Assuming you use GitOps to deploy the routed pods, you might use the following template to deploy the gateway pod: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway namespace : default labels : spec : interval : 5m chart : spec : # renovate: registryUrl=https://k8s-at-spiti.com/charts/ chart : pod-gateway version : 2.0.0 sourceRef : kind : HelmRepository name : k8s-at-spiti-charts namespace : flux-system interval : 5m # See https://github.com/k8s-at-spiti/charts/blob/master/charts/pod-gateway/values.yaml values : routed_namespaces : - vpn The above should deploy a gateway and a gateway-hook: the gateway-hook has the task of modifying created PODs in the vpn namespace to be configured to use the VPN gateway. You will find more details in its git repository . the gateway provides a VXLAN tunnel, a DHCP server and a DNS server for client pods to connect to. Optionally it also runs the VPN client. You will find more details in its git repository .","title":"pod-gateway Helm release"},{"location":"guides/pod-gateway/#test-deployment","text":"Then you can deploy a test deployment to test it: # TestDeployment.yaml --- apiVersion : apps/v1 kind : Deployment metadata : name : terminal namespace : vpn labels : app : terminal spec : replicas : 1 selector : matchLabels : app : terminal template : metadata : labels : app : terminal spec : containers : - name : alpine image : alpine command : - /bin/sh - -c - while true; do sleep 600 & wait $!; done If you exec into it you should see the traffic being routed through the gateway: root@k3s1:~# kubectl exec -ti -n vpn deploy/terminal -- sh / # ip route default via 172 .16.0.1 dev vxlan0 10 .0.0.0/8 via 10 .0.2.1 dev eth0 10 .0.2.0/24 dev eth0 scope link src 10 .0.2.174 172 .16.0.0/24 dev vxlan0 scope link src 172 .16.0.133 / # ip addr 1 : lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00 :00:00:00:00:00 brd 00 :00:00:00:00:00 inet 127 .0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 3 : eth0@if228: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1450 qdisc noqueue state UP link/ether 6a:e8:0b:31:eb:3e brd ff:ff:ff:ff:ff:ff inet 10 .0.2.174/24 brd 10 .0.2.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::68e8:bff:fe31:eb3e/64 scope link valid_lft forever preferred_lft forever 4 : vxlan0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1400 qdisc noqueue state UNKNOWN qlen 1000 link/ether 06 :81:5d:8c:4a:15 brd ff:ff:ff:ff:ff:ff inet 172 .16.0.133/24 brd 172 .16.0.255 scope global vxlan0 valid_lft forever preferred_lft forever inet6 fe80::481:5dff:fe8c:4a15/64 scope link valid_lft forever preferred_lft forever / # cat /etc/resolv.conf nameserver 172 .16.0.1 / # ping -c1 example.com 64 bytes from 93 .184.216.34: seq = 0 ttl = 115 time = 36 .366 ms --- example.com ping statistics --- 1 packets transmitted, 1 packets received, 0 % packet loss round-trip min/avg/max = 36 .366/36.366/36.366 ms The important part is that the default gateway and the DNS are set to 172.16.0.1 which is the default IP of the gateway POD in the vlxlan network. If this is the case then you are ready for the (optional) VPN setup. If the ping does not work and you are using Calico please check the Calico section bellow.","title":"Test deployment"},{"location":"guides/pod-gateway/#network-policy","text":"For additional precaution, you should deploy a network policy into each routed namespace to prevent traffic leaving the K8S cluster without passing through the pod gateway. This way, even in case of setup errors, you will not leak any traffic. # NetworkPolicy.yaml --- kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : vpn-namespace spec : podSelector : {} ingress : - from : # Only allow ingress from K8S - ipBlock : cidr : 10.0.0.0/8 egress : - to : # Only allow egress to K8S - ipBlock : cidr : 10.0.0.0/8 policyTypes : - Ingress - Egress","title":"Network Policy"},{"location":"guides/pod-gateway/#additional-configuration","text":"","title":"Additional Configuration"},{"location":"guides/pod-gateway/#vpn","text":"You will need to enable the vpn sidecar in the helm release settings: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... addons : vpn : enabled : true # You might use `openvpn` or `wireguard` type : openvpn openvpn : # VPN settings stored in secret `vpnConfig`. The secret mus have a key # a key called `vpnConfigfile` with the openvpn/wireguard config files in them configFileSecret : openvpn livenessProbe : exec : # In the example bellow the VPN output is in Belgic (BE) - change appropiatly command : - sh - -c - if [ $(wget -q -O- https://ipinfo.io/country) == 'BE' ]; then exit 0; else exit $?; fi initialDelaySeconds : 30 periodSeconds : 60 failureThreshold : 1 networkPolicy : enabled : true egress : - to : - ipBlock : cidr : 0.0.0.0/0 ports : # VPN traffic port - change if your provider uses a different port - port : 443 protocol : UDP - to : # Allow traffic within K8S - change if your K8S cluster uses a different CIDR - ipBlock : cidr : 10.0.0.0/8 settings : # tun0 for openvpn, wg0 for wireguard VPN_INTERFACE : tun0 # Prevent non VPN traffic to leave the gateway VPN_BLOCK_OTHER_TRAFFIC : true # If VPN_BLOCK_OTHER_TRAFFIC is true, allow VPN traffic over this port VPN_TRAFFIC_PORT : 443 # Traffic to these IPs will be send through the K8S gateway # change if your K8S cluster or home network uses a different CIDR VPN_LOCAL_CIDRS : \"10.0.0.0/8 192.168.0.0/16\"","title":"VPN"},{"location":"guides/pod-gateway/#exposing-routed-pod-ports-from-the-gateway","text":"You can expose individual ports of routed PODs thought he pod gateway. This is specially useful if you need to expose PODs to the Internet through the VPN server. For example, you can expose the torrent port or a web server. You need to ensure the exposed routed pod has a static name. If you use the k8s-at-spiti charts, this is done by setting the hostname value: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : qbittorrent spec : ... values : hostname : torrent # needed for an static IP Then you can expose the port by extending the pod-gateway helm-release: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... # -- settings to expose ports, usually through a VPN provider. # NOTE : if you change it you will need to manually restart the gateway POD publicPorts : - hostname : torrent #hostname assigned to the pod IP : 10 # must be an integer between 2 and VXLAN_GATEWAY_FIRST_DYNAMIC_IP (20 by default) ports : - type : udp port : 18289 - type : tcp port : 18289","title":"Exposing routed pod ports from the gateway"},{"location":"guides/pod-gateway/#calico","text":"Calico only configures a single default gateway and not dedicated rules for traffic within the K8S cluster. So you will need to add that explictly by extending the pod-gateway helm chart deployment: # HelmRelease.yaml --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : vpn-gateway spec : ... values : ... settings : # Route internal K8s and local home traffic in to the defaullt K8S gateway NOT_ROUTED_TO_GATEWAY_CIDRS : \"172.22.0.0/12 <local home network CIDR>\" VPN_LOCAL_CIDRS : \"172.22.0.0/12 <local home network CIDR>\" # Use a different VXLAN network segment that does not conflict with the above VXLAN_IP_NETWORK : \"192.168.242.0/24\"","title":"Calico"},{"location":"guides/pod-gateway/#how-to-debug","text":"","title":"How to debug"},{"location":"guides/pod-gateway/#connectivity","text":"From the gateway and routed pods check you can ping the following: K8S DNS IP (check an stardard port to find out) if this fails, then likelly you forgot setting NOT_ROUTED_TO_GATEWAY_CIDRS . Please read the Calico setup. from the routed pod, the gateway IP (172.16.0.1 by default) if this fails, there is problem with the VXLAN setup and more debugging is needed from the gateway pod, the routed pod if this fails, there is problem with the VXLAN setup and more debugging is needed an IP in your home network if this fails, then likelly you forgot setting VPN_LOCAL_CIDRS to include your K8S CIDR - the default 10.0.0.0/8 works in Flannel (k3s default) but not in Calico. a hostname in Internet without VPN: if this fails, you might have set the VPN_BLOCK_OTHER_TRAFFIC variable to true with VPN: if this fails, check the following: output of the wireguard VPN_INTERFACE - must be wg0 for wireguard VPN_TRAFFIC_PORT - the default, 443 works for some openvpn servers VPN_LOCAL_CIDRS - it must include the K8S and local home CIDRs a hostname if this fails, there is problem with the DNS resolution. More debugging is needed","title":"Connectivity"},{"location":"guides/pod-gateway/#routed-pod-fails-to-init","text":"If the test pod/terminal pod fails to get passed the init container, check the logs from the init container kubectl -n vpn logs terminal-xdfexampleaex-pvfpc gateway-init If you see + GATEWAY_IP=';; connection timed out; no servers could be reached' , try setting the NOT_ROUTED_TO_GATEWAY_CIDRS: with your cluster cidr and service cidrs. For example, if you're running k3s with the default flannel install , it will be NOT_ROUTED_TO_GATEWAY_CIDRS: \"10.42.0.0/16 10.43.0.0/16\"","title":"Routed Pod Fails to init"},{"location":"guides/pod-gateway/#collecting-more-debug","text":"Collect the following information from the routed and gateway pods: ip route ip addr cat /etc/resolv.conf ping -c1 example.com You can compare with the output of the deployment example above. If you are not able to tell what the problem is you might open an issue for chart pod-gateway . Please attach there the connectivity test results and the additional debug information.","title":"Collecting more debug"},{"location":"guides/recyclarr/","text":"Introduction \u00b6 This is a guide on how to use native k8s CronJob s to run Recyclarr against your existing Sonarr and Radarr installations in two easy steps. Creating a ConfigMap \u00b6 Step one is to create a configmap based on the documentation . Note This example is based on the default configuration. You will want to make changes based on the documentation above. If you are referencing the example below, replace the API keys with the keys for your installation. You can find them under Settings > General . You must also replace the base_url entries with urls that are reachable from the pod that the cronjob will create. This is actually pretty easy. If you are running Sonarr or Radarr inside the same cluster as Recyclarr, you should be able to use the addresses in the examples by replacing default with the correct namespace. If you aren't sure of the exact naming, kubectl get services -A will list all of the services in the cluster. And don't forget the port! # configmap.yaml --- apiVersion : v1 kind : ConfigMap metadata : name : recyclarr-config namespace : default labels : app.kubernetes.io/name : recyclarr app.kubernetes.io/instance : recyclarr data : recyclarr.yml : | sonarr: - base_url: http://sonarr.default.svc:8989 api_key: f7e74ba6c80046e39e076a27af5a8444 # Quality Definition Settings quality_definition: hybrid # Release Profile Settings release_profiles: - trash_ids: - d428eda85af1df8904b4bbe4fc2f537c # Anime - First release profile - 6cd9e10bb5bb4c63d2d7cd3279924c7b # Anime - Second release profile strict_negative_scores: true tags: [anime] - trash_ids: - EBC725268D687D588A20CBC5F97E538B # Low Quality Groups - 1B018E0C53EC825085DD911102E2CA36 # Release Sources (Streaming Service) - 71899E6C303A07AF0E4746EFF9873532 # P2P Groups + Repack/Proper strict_negative_scores: false tags: [tv] - trash_ids: [76e060895c5b8a765c310933da0a5357] # Optionals filter: include: - 436f5a7d08fbf02ba25cb5e5dfe98e55 # Ignore Dolby Vision without HDR10 fallback - f3f0f3691c6a1988d4a02963e69d11f2 # Ignore The Group -SCENE tags: [tv] radarr: - base_url: http://radarr.default.svc:7878 api_key: bf99da49d0b0488ea34e4464aa63a0e5 # Quality Definition Settings quality_definition: type: movie preferred_ratio: 0.5 # Custom Format Settings delete_old_custom_formats: false custom_formats: - names: - BR-DISK - EVO (no WEB-DL) - LQ - x265 (720/1080p) - 3D quality_profiles: - name: HD-1080p - name: HD-720p2 score: -1000 - names: - TrueHD ATMOS - DTS X quality_profiles: - name: SD Creating the CronJob \u00b6 The second and last step is to create a CronJob . This example will run every 8 hours. Note This is intentionally set to run in preview mode. Remove the ,\"-p\" from the args to apply changes to Radarr. # radarr-cronjob.yaml --- apiVersion : batch/v1 kind : CronJob metadata : name : recyclarr-radarr namespace : default labels : app.kubernetes.io/name : recyclarr app.kubernetes.io/instance : recyclarr-radarr app.kubernetes.io/version : \"2.2\" spec : schedule : \"0 */3 * * *\" # every 8 hours concurrencyPolicy : Replace successfulJobsHistoryLimit : 21 # 1 week failedJobsHistoryLimit : 3 jobTemplate : spec : template : spec : restartPolicy : Never volumes : - name : recyclarr-config configMap : name : recyclarr-config containers : - name : recyclarr image : ghcr.io/recyclarr/recyclarr:2.2 command : [ \"recyclarr\" ] args : [ \"radarr\" , \"-p\" ] volumeMounts : - mountPath : \"/config/recyclarr.yml\" name : recyclarr-config subPath : recyclarr.yml","title":"Updating Sonarr/Radarr with Recyclarr"},{"location":"guides/recyclarr/#introduction","text":"This is a guide on how to use native k8s CronJob s to run Recyclarr against your existing Sonarr and Radarr installations in two easy steps.","title":"Introduction"},{"location":"guides/recyclarr/#creating-a-configmap","text":"Step one is to create a configmap based on the documentation . Note This example is based on the default configuration. You will want to make changes based on the documentation above. If you are referencing the example below, replace the API keys with the keys for your installation. You can find them under Settings > General . You must also replace the base_url entries with urls that are reachable from the pod that the cronjob will create. This is actually pretty easy. If you are running Sonarr or Radarr inside the same cluster as Recyclarr, you should be able to use the addresses in the examples by replacing default with the correct namespace. If you aren't sure of the exact naming, kubectl get services -A will list all of the services in the cluster. And don't forget the port! # configmap.yaml --- apiVersion : v1 kind : ConfigMap metadata : name : recyclarr-config namespace : default labels : app.kubernetes.io/name : recyclarr app.kubernetes.io/instance : recyclarr data : recyclarr.yml : | sonarr: - base_url: http://sonarr.default.svc:8989 api_key: f7e74ba6c80046e39e076a27af5a8444 # Quality Definition Settings quality_definition: hybrid # Release Profile Settings release_profiles: - trash_ids: - d428eda85af1df8904b4bbe4fc2f537c # Anime - First release profile - 6cd9e10bb5bb4c63d2d7cd3279924c7b # Anime - Second release profile strict_negative_scores: true tags: [anime] - trash_ids: - EBC725268D687D588A20CBC5F97E538B # Low Quality Groups - 1B018E0C53EC825085DD911102E2CA36 # Release Sources (Streaming Service) - 71899E6C303A07AF0E4746EFF9873532 # P2P Groups + Repack/Proper strict_negative_scores: false tags: [tv] - trash_ids: [76e060895c5b8a765c310933da0a5357] # Optionals filter: include: - 436f5a7d08fbf02ba25cb5e5dfe98e55 # Ignore Dolby Vision without HDR10 fallback - f3f0f3691c6a1988d4a02963e69d11f2 # Ignore The Group -SCENE tags: [tv] radarr: - base_url: http://radarr.default.svc:7878 api_key: bf99da49d0b0488ea34e4464aa63a0e5 # Quality Definition Settings quality_definition: type: movie preferred_ratio: 0.5 # Custom Format Settings delete_old_custom_formats: false custom_formats: - names: - BR-DISK - EVO (no WEB-DL) - LQ - x265 (720/1080p) - 3D quality_profiles: - name: HD-1080p - name: HD-720p2 score: -1000 - names: - TrueHD ATMOS - DTS X quality_profiles: - name: SD","title":"Creating a ConfigMap"},{"location":"guides/recyclarr/#creating-the-cronjob","text":"The second and last step is to create a CronJob . This example will run every 8 hours. Note This is intentionally set to run in preview mode. Remove the ,\"-p\" from the args to apply changes to Radarr. # radarr-cronjob.yaml --- apiVersion : batch/v1 kind : CronJob metadata : name : recyclarr-radarr namespace : default labels : app.kubernetes.io/name : recyclarr app.kubernetes.io/instance : recyclarr-radarr app.kubernetes.io/version : \"2.2\" spec : schedule : \"0 */3 * * *\" # every 8 hours concurrencyPolicy : Replace successfulJobsHistoryLimit : 21 # 1 week failedJobsHistoryLimit : 3 jobTemplate : spec : template : spec : restartPolicy : Never volumes : - name : recyclarr-config configMap : name : recyclarr-config containers : - name : recyclarr image : ghcr.io/recyclarr/recyclarr:2.2 command : [ \"recyclarr\" ] args : [ \"radarr\" , \"-p\" ] volumeMounts : - mountPath : \"/config/recyclarr.yml\" name : recyclarr-config subPath : recyclarr.yml","title":"Creating the CronJob"},{"location":"our-container-images/configuration/","text":"Configuration \u00b6 The following configuration is available across all the k8s@spiti container images. Environment Variables \u00b6 Name Default Description UMASK 0002 Set the default creation permission mode of files WAIT_FOR_VPN false EXTRA_ARGS Additional arguments to pass to the application TZ UTC Timezone (e.g. America/New_York ) Volumes \u00b6 Path Description /app Application install directory /config Application configuration directory User \u00b6 User ID kah 568","title":"Configuration"},{"location":"our-container-images/configuration/#configuration","text":"The following configuration is available across all the k8s@spiti container images.","title":"Configuration"},{"location":"our-container-images/configuration/#environment-variables","text":"Name Default Description UMASK 0002 Set the default creation permission mode of files WAIT_FOR_VPN false EXTRA_ARGS Additional arguments to pass to the application TZ UTC Timezone (e.g. America/New_York )","title":"Environment Variables"},{"location":"our-container-images/configuration/#volumes","text":"Path Description /app Application install directory /config Application configuration directory","title":"Volumes"},{"location":"our-container-images/configuration/#user","text":"User ID kah 568","title":"User"},{"location":"our-container-images/introduction/","text":"Introduction \u00b6 Running applications in Kubernetes is a bit different that in a regular Docker Swarm or Docker Compose setup. Our container images are tailored for running in Kubernetes. Purpose \u00b6 The goal of this project and the container images are to support Semantic Versioning and multiple architectures. We try to keep a KISS principle when building these images, which means no s6-overlay , are all built on top of ubuntu:focal , and are only amd64 and arm64 architectures. Deprecations \u00b6 Container images that were once here but now are gone it is likely that the application... developers are supporting their own images and conforming to Semantic Versioning , or has been replaced with a different application, or maintenance cost it too high to continue to build images for, or is no longer maintained","title":"Introduction"},{"location":"our-container-images/introduction/#introduction","text":"Running applications in Kubernetes is a bit different that in a regular Docker Swarm or Docker Compose setup. Our container images are tailored for running in Kubernetes.","title":"Introduction"},{"location":"our-container-images/introduction/#purpose","text":"The goal of this project and the container images are to support Semantic Versioning and multiple architectures. We try to keep a KISS principle when building these images, which means no s6-overlay , are all built on top of ubuntu:focal , and are only amd64 and arm64 architectures.","title":"Purpose"},{"location":"our-container-images/introduction/#deprecations","text":"Container images that were once here but now are gone it is likely that the application... developers are supporting their own images and conforming to Semantic Versioning , or has been replaced with a different application, or maintenance cost it too high to continue to build images for, or is no longer maintained","title":"Deprecations"},{"location":"our-container-images/permissions/","text":"Permissions \u00b6 With Kubernetes, s6-overlay is not needed. Instead Kubernetes can use Security Context on either the pod or container to tell what the container should run as, and/or what permissions files should be written as. Managing Permissions \u00b6 There are several different methods you can use to make these containers have write access to your file storage. Note Our images use a default user/group id of 568 . The user and group ids cannot be changed at the container runtime. Security Contexts method \u00b6 You can change the Kubernetes Security Contexts to allow the container to have permissions to write to your file storage. In our Helm charts this can be accomplished by setting the following option in you Helm values. Note According to the Kubernetes docs fsGroup will chown the volume with the runAsUser and runAsGroup IDs. As such this option should only have to be set once or it may cause your pod to take long to start each time it is started. podSecurityContext : runAsUser : 568 runAsGroup : 568 fsGroup : 568 Optional configuration \u00b6 To prevent issues with long start-up times using this method, you can specify fsGroupChangePolicy with one of the following: Always Instructs Kubernetes to chown the volume each time the pod starts OnRootMismatch Instructs Kubernetes to chown the volume only if the permissions on the root of the volume do not already match This is typically much faster than Always Note This is an alpha feature in Kubernetes 1.18-1.19, and must be manually enabled . This became a beta feature in Kubernetes 1.20, and as such is enabled by default in Kubernetes 1.20+ Example: podSecurityContext : runAsUser : 568 runAsGroup : 568 fsGroup : 568 fsGroupChangePolicy : \"OnRootMismatch\" initContainer method \u00b6 Implement a initContainer that runs as root to automatically chown the volume's data. initContainers : - name : update-volume-permission image : busybox command : [ \"sh\" , \"-c\" , \"chown -R 568:568 /config\" ] volumeMounts : - name : config mountPath : /config securityContext : runAsUser : 0 Direct volume method \u00b6 Warning This step can be a bit complicated if you are not very familiar with your storage interface. If you can mount the volume's config without the pod running. You can run chown -R 568:568 <path-to-the mounted-volume-on-the host> . As this method varies greatly depending on your CSI driver, we won't be mentioning how to accomplish this.","title":"Permissions"},{"location":"our-container-images/permissions/#permissions","text":"With Kubernetes, s6-overlay is not needed. Instead Kubernetes can use Security Context on either the pod or container to tell what the container should run as, and/or what permissions files should be written as.","title":"Permissions"},{"location":"our-container-images/permissions/#managing-permissions","text":"There are several different methods you can use to make these containers have write access to your file storage. Note Our images use a default user/group id of 568 . The user and group ids cannot be changed at the container runtime.","title":"Managing Permissions"},{"location":"our-container-images/permissions/#security-contexts-method","text":"You can change the Kubernetes Security Contexts to allow the container to have permissions to write to your file storage. In our Helm charts this can be accomplished by setting the following option in you Helm values. Note According to the Kubernetes docs fsGroup will chown the volume with the runAsUser and runAsGroup IDs. As such this option should only have to be set once or it may cause your pod to take long to start each time it is started. podSecurityContext : runAsUser : 568 runAsGroup : 568 fsGroup : 568","title":"Security Contexts method"},{"location":"our-container-images/permissions/#optional-configuration","text":"To prevent issues with long start-up times using this method, you can specify fsGroupChangePolicy with one of the following: Always Instructs Kubernetes to chown the volume each time the pod starts OnRootMismatch Instructs Kubernetes to chown the volume only if the permissions on the root of the volume do not already match This is typically much faster than Always Note This is an alpha feature in Kubernetes 1.18-1.19, and must be manually enabled . This became a beta feature in Kubernetes 1.20, and as such is enabled by default in Kubernetes 1.20+ Example: podSecurityContext : runAsUser : 568 runAsGroup : 568 fsGroup : 568 fsGroupChangePolicy : \"OnRootMismatch\"","title":"Optional configuration"},{"location":"our-container-images/permissions/#initcontainer-method","text":"Implement a initContainer that runs as root to automatically chown the volume's data. initContainers : - name : update-volume-permission image : busybox command : [ \"sh\" , \"-c\" , \"chown -R 568:568 /config\" ] volumeMounts : - name : config mountPath : /config securityContext : runAsUser : 0","title":"initContainer method"},{"location":"our-container-images/permissions/#direct-volume-method","text":"Warning This step can be a bit complicated if you are not very familiar with your storage interface. If you can mount the volume's config without the pod running. You can run chown -R 568:568 <path-to-the mounted-volume-on-the host> . As this method varies greatly depending on your CSI driver, we won't be mentioning how to accomplish this.","title":"Direct volume method"},{"location":"our-container-images/development/base-images/","text":"Base images \u00b6 The k8s@spiti base images are meant to be used as an image to build all other app container images on top of. Distributions \u00b6 The following distributions are available as a base image: Ubuntu Alpine The source code can be found here . shim scripts \u00b6 shim scripts have been added to the Ubuntu base image to perform startup tasks before the app is launched. Note The shim scripts must be sourced in the entrypoint.sh file in order to work. config.sh \u00b6 The config.sh shim script is used to create the /config folder. umask.sh \u00b6 The umask.sh shim script is used to set the setting of a mask that controls how file permissions are set for newly created files. vpn.sh \u00b6 The vpn.sh shim script is used to allow the container to wait for the VPN connection before running the app. Tini \u00b6 The base images use the Tini init in order to create the smallest image possible while still waiting for a child to exit all the while reaping zombies and performing signal forwarding.","title":"Base images"},{"location":"our-container-images/development/base-images/#base-images","text":"The k8s@spiti base images are meant to be used as an image to build all other app container images on top of.","title":"Base images"},{"location":"our-container-images/development/base-images/#distributions","text":"The following distributions are available as a base image: Ubuntu Alpine The source code can be found here .","title":"Distributions"},{"location":"our-container-images/development/base-images/#shim-scripts","text":"shim scripts have been added to the Ubuntu base image to perform startup tasks before the app is launched. Note The shim scripts must be sourced in the entrypoint.sh file in order to work.","title":"shim scripts"},{"location":"our-container-images/development/base-images/#configsh","text":"The config.sh shim script is used to create the /config folder.","title":"config.sh"},{"location":"our-container-images/development/base-images/#umasksh","text":"The umask.sh shim script is used to set the setting of a mask that controls how file permissions are set for newly created files.","title":"umask.sh"},{"location":"our-container-images/development/base-images/#vpnsh","text":"The vpn.sh shim script is used to allow the container to wait for the VPN connection before running the app.","title":"vpn.sh"},{"location":"our-container-images/development/base-images/#tini","text":"The base images use the Tini init in order to create the smallest image possible while still waiting for a child to exit all the while reaping zombies and performing signal forwarding.","title":"Tini"},{"location":"our-container-images/development/build/","text":"Build \u00b6 Building of the container images need to be done from the root directory of the repository so that the app files, such as entrypoint.sh , can be properly copied into the image. Docker Buildx \u00b6 Docker Buildx is required to build multi-platform images. qemu-user-static may be used to enable an execution of different multi-architecture containers by QEMU and binfmt_misc. docker run --rm --privileged multiarch/qemu-user-static --reset -p yes Task \u00b6 Task may be used to semi-automate the building and loading of a container image. The APP parameter will need to be set in order to tell Task which app needs to be processed. Run the following command from the root directory of the repository. task build APP = <app name> Other tasks can be listed by running task -l . Manual \u00b6 The container images can be built and loaded manually by using the Docker cli from the repository root directory. Build \u00b6 docker buildx build --build-arg VERSION = $( cat ./apps/<app name>/VERSION ) --platform $( cat ./apps/<app name>/PLATFORM ) -f ./apps/<app name>/Dockerfile . Load \u00b6 docker buildx build -t <app name>:test --build-arg VERSION = $( cat ./apps/<app name>/VERSION ) -f ./apps/<app name>/Dockerfile . --load","title":"Build"},{"location":"our-container-images/development/build/#build","text":"Building of the container images need to be done from the root directory of the repository so that the app files, such as entrypoint.sh , can be properly copied into the image.","title":"Build"},{"location":"our-container-images/development/build/#docker-buildx","text":"Docker Buildx is required to build multi-platform images. qemu-user-static may be used to enable an execution of different multi-architecture containers by QEMU and binfmt_misc. docker run --rm --privileged multiarch/qemu-user-static --reset -p yes","title":"Docker Buildx"},{"location":"our-container-images/development/build/#task","text":"Task may be used to semi-automate the building and loading of a container image. The APP parameter will need to be set in order to tell Task which app needs to be processed. Run the following command from the root directory of the repository. task build APP = <app name> Other tasks can be listed by running task -l .","title":"Task"},{"location":"our-container-images/development/build/#manual","text":"The container images can be built and loaded manually by using the Docker cli from the repository root directory.","title":"Manual"},{"location":"our-container-images/development/build/#build_1","text":"docker buildx build --build-arg VERSION = $( cat ./apps/<app name>/VERSION ) --platform $( cat ./apps/<app name>/PLATFORM ) -f ./apps/<app name>/Dockerfile .","title":"Build"},{"location":"our-container-images/development/build/#load","text":"docker buildx build -t <app name>:test --build-arg VERSION = $( cat ./apps/<app name>/VERSION ) -f ./apps/<app name>/Dockerfile . --load","title":"Load"},{"location":"our-container-images/development/creating-a-new-container-image/","text":"Creating a new container image \u00b6 Dependencies \u00b6 If you would like to help create new container images, there's a few tools you will need: Docker buildx jq goss & dgoss task (optional) Creating a new container image \u00b6 Currently, there isn't a template to start from and so you can either start with an already existing app or you can start from scratch. Create a folder in in the apps folder with the name of the container image. Add the following files to the container image folder. Folder structure \u00b6 apps/app name/ \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 entrypoint.sh \u251c\u2500\u2500 goss.yaml \u251c\u2500\u2500 latest-version.sh \u251c\u2500\u2500 PLATFORM \u251c\u2500\u2500 shim \u2502 \u2514\u2500\u2500 shim-script.sh \u2514\u2500\u2500 VERSION Dockerfile \u00b6 See Dockerfile . PLATFORM \u00b6 The PLATFORM file is used by the to build multi-platform container images. At a minimum, the following platforms should be supported by the container image: amd64 arm64 linux/amd64,linux/arm64 VERSION \u00b6 The VERSION file is used by to determine the version of the app with which to build the container image. The VERSION file may be updated manually by running the latest-version.sh script. ./latest-version.sh > VERSION entrypoint.sh \u00b6 The entrypoint.sh file is a standard Docker entrypoint file . This file will most likely be custom for each app. It is recommended to take a look at other container images or other versions of the Docker image for the app. A good source might be searching Docker Hub . The only difference is the sourcing of the base image shim scripts which are added to the base image. #shellcheck disable=SC1091 source \"/shim/umask.sh\" source \"/shim/vpn.sh\" If the app has custom shim scripts , be sure to source those as well. source \"/shim/<app name>-preferences.sh\" Ensure to append the ${EXTRA_ARGS} environmental variable to the end of the exec command so that extra args can be passed to the container by the helm chart. exec /app/foo ${ EXTRA_ARGS } goss.yaml \u00b6 The goss.yaml file is used to perform a health check on the container image using goss . Be sure to update the process name, port number(s) and title . See the manual for more options. --- process : Lidarr : running : true port : tcp6:8686 : listening : true http : http://localhost:8686 : status : 200 body : - '<title>Lidarr</title>' latest-version.sh \u00b6 The latest-version.sh script is used to get the latest version of the app. This script is custom for each app and so it will need to be developed for each app. Below is an example of one. #!/usr/bin/env bash # Get the version using jq from the json response. version = $( curl -sX GET \"https://lidarr.servarr.com/v1/update/nightly/changes?os=linux\" | jq --raw-output '.[0].version' ) # Strip the v from the beginning of version if it exists. version = \" ${ version #*v } \" # Remove release and dash from the beginning of version if it exists. version = \" ${ version #*release- } \" # Print the verion without a new line (\\n) character. printf \"%s\" \" ${ version } \" shim scripts \u00b6 Custom startup scripts should be added to the shim folder that needs to be run before the app is started. Create a shim folder in the app name folder then add the shim scripts to the shim folder. Then add following line to the Dockerfile to copy the scripts over to the image. Then be sure to add a source line to the entrypoint.sh script . See plex for reference.","title":"Creating a new container image"},{"location":"our-container-images/development/creating-a-new-container-image/#creating-a-new-container-image","text":"","title":"Creating a new container image"},{"location":"our-container-images/development/creating-a-new-container-image/#dependencies","text":"If you would like to help create new container images, there's a few tools you will need: Docker buildx jq goss & dgoss task (optional)","title":"Dependencies"},{"location":"our-container-images/development/creating-a-new-container-image/#creating-a-new-container-image_1","text":"Currently, there isn't a template to start from and so you can either start with an already existing app or you can start from scratch. Create a folder in in the apps folder with the name of the container image. Add the following files to the container image folder.","title":"Creating a new container image"},{"location":"our-container-images/development/creating-a-new-container-image/#folder-structure","text":"apps/app name/ \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 entrypoint.sh \u251c\u2500\u2500 goss.yaml \u251c\u2500\u2500 latest-version.sh \u251c\u2500\u2500 PLATFORM \u251c\u2500\u2500 shim \u2502 \u2514\u2500\u2500 shim-script.sh \u2514\u2500\u2500 VERSION","title":"Folder structure"},{"location":"our-container-images/development/creating-a-new-container-image/#dockerfile","text":"See Dockerfile .","title":"Dockerfile"},{"location":"our-container-images/development/creating-a-new-container-image/#platform","text":"The PLATFORM file is used by the to build multi-platform container images. At a minimum, the following platforms should be supported by the container image: amd64 arm64 linux/amd64,linux/arm64","title":"PLATFORM"},{"location":"our-container-images/development/creating-a-new-container-image/#version","text":"The VERSION file is used by to determine the version of the app with which to build the container image. The VERSION file may be updated manually by running the latest-version.sh script. ./latest-version.sh > VERSION","title":"VERSION"},{"location":"our-container-images/development/creating-a-new-container-image/#entrypointsh","text":"The entrypoint.sh file is a standard Docker entrypoint file . This file will most likely be custom for each app. It is recommended to take a look at other container images or other versions of the Docker image for the app. A good source might be searching Docker Hub . The only difference is the sourcing of the base image shim scripts which are added to the base image. #shellcheck disable=SC1091 source \"/shim/umask.sh\" source \"/shim/vpn.sh\" If the app has custom shim scripts , be sure to source those as well. source \"/shim/<app name>-preferences.sh\" Ensure to append the ${EXTRA_ARGS} environmental variable to the end of the exec command so that extra args can be passed to the container by the helm chart. exec /app/foo ${ EXTRA_ARGS }","title":"entrypoint.sh"},{"location":"our-container-images/development/creating-a-new-container-image/#gossyaml","text":"The goss.yaml file is used to perform a health check on the container image using goss . Be sure to update the process name, port number(s) and title . See the manual for more options. --- process : Lidarr : running : true port : tcp6:8686 : listening : true http : http://localhost:8686 : status : 200 body : - '<title>Lidarr</title>'","title":"goss.yaml"},{"location":"our-container-images/development/creating-a-new-container-image/#latest-versionsh","text":"The latest-version.sh script is used to get the latest version of the app. This script is custom for each app and so it will need to be developed for each app. Below is an example of one. #!/usr/bin/env bash # Get the version using jq from the json response. version = $( curl -sX GET \"https://lidarr.servarr.com/v1/update/nightly/changes?os=linux\" | jq --raw-output '.[0].version' ) # Strip the v from the beginning of version if it exists. version = \" ${ version #*v } \" # Remove release and dash from the beginning of version if it exists. version = \" ${ version #*release- } \" # Print the verion without a new line (\\n) character. printf \"%s\" \" ${ version } \"","title":"latest-version.sh"},{"location":"our-container-images/development/creating-a-new-container-image/#shim-scripts","text":"Custom startup scripts should be added to the shim folder that needs to be run before the app is started. Create a shim folder in the app name folder then add the shim scripts to the shim folder. Then add following line to the Dockerfile to copy the scripts over to the image. Then be sure to add a source line to the entrypoint.sh script . See plex for reference.","title":"shim scripts"},{"location":"our-container-images/development/dockerfile/","text":"Dockerfile \u00b6 The Dockerfile file is a standard Dockerfile with a few additions. Base images \u00b6 Be sure to use one of the base k8s@spiti images for the FROM instruction. Ubuntu \u00b6 # hadolint ignore=DL3007 FROM ghcr.io/k8s-at-spiti/ubuntu:latest Alpine \u00b6 # hadolint ignore=DL3007 FROM ghcr.io/k8s-at-spiti/alpine:latest package_info \u00b6 Also, be sure to add the package_info file to the image so that others can find the source of the container image. RUN printf \"UpdateMethod=docker\\nPackageVersion=%s\\nPackageAuthor=[Team k8s-at-spiti](https://github.com/k8s-at-spiti)\" \" ${ VERSION } \" > /app/package_info UMASK and ca-certificates \u00b6 Ensure that you change the permissions of the /app folder, set the UMASK environmental variable, and update ca-certificates . RUN \\ chmod -R u = rwX,go = rX /app \\ && printf \"umask %d\" \" ${ UMASK } \" >> /etc/bash.bashrc \\ && update-ca-certificates COPY \u00b6 When using the COPY instruction, be sure to start from the root of the repository so that the github actions can build the container image properly. COPY ./apps/<app name>/entrypoint.sh /entrypoint.sh USER \u00b6 Be sure set the USER to kah . See Configuration for details. USER kah shim scripts \u00b6 If custom shim scripts are used, be sure to copy them to the container image. COPY ./apps/<app name>/shim/<app name>-preferences.sh /shim/<app name>-preferences.sh Linting \u00b6 Hadolint needs to be used to lint the Dockerfile before publishing. Hadolint is also used as a check in the Apps - Build, Test, Push github action hadolint Dockerfile Best practices \u00b6 Be sure to use best practices up for writing Dockerfiles in order to reduce the container image size.","title":"Dockerfile"},{"location":"our-container-images/development/dockerfile/#dockerfile","text":"The Dockerfile file is a standard Dockerfile with a few additions.","title":"Dockerfile"},{"location":"our-container-images/development/dockerfile/#base-images","text":"Be sure to use one of the base k8s@spiti images for the FROM instruction.","title":"Base images"},{"location":"our-container-images/development/dockerfile/#ubuntu","text":"# hadolint ignore=DL3007 FROM ghcr.io/k8s-at-spiti/ubuntu:latest","title":"Ubuntu"},{"location":"our-container-images/development/dockerfile/#alpine","text":"# hadolint ignore=DL3007 FROM ghcr.io/k8s-at-spiti/alpine:latest","title":"Alpine"},{"location":"our-container-images/development/dockerfile/#package_info","text":"Also, be sure to add the package_info file to the image so that others can find the source of the container image. RUN printf \"UpdateMethod=docker\\nPackageVersion=%s\\nPackageAuthor=[Team k8s-at-spiti](https://github.com/k8s-at-spiti)\" \" ${ VERSION } \" > /app/package_info","title":"package_info"},{"location":"our-container-images/development/dockerfile/#umask-and-ca-certificates","text":"Ensure that you change the permissions of the /app folder, set the UMASK environmental variable, and update ca-certificates . RUN \\ chmod -R u = rwX,go = rX /app \\ && printf \"umask %d\" \" ${ UMASK } \" >> /etc/bash.bashrc \\ && update-ca-certificates","title":"UMASK and ca-certificates"},{"location":"our-container-images/development/dockerfile/#copy","text":"When using the COPY instruction, be sure to start from the root of the repository so that the github actions can build the container image properly. COPY ./apps/<app name>/entrypoint.sh /entrypoint.sh","title":"COPY"},{"location":"our-container-images/development/dockerfile/#user","text":"Be sure set the USER to kah . See Configuration for details. USER kah","title":"USER"},{"location":"our-container-images/development/dockerfile/#shim-scripts","text":"If custom shim scripts are used, be sure to copy them to the container image. COPY ./apps/<app name>/shim/<app name>-preferences.sh /shim/<app name>-preferences.sh","title":"shim scripts"},{"location":"our-container-images/development/dockerfile/#linting","text":"Hadolint needs to be used to lint the Dockerfile before publishing. Hadolint is also used as a check in the Apps - Build, Test, Push github action hadolint Dockerfile","title":"Linting"},{"location":"our-container-images/development/dockerfile/#best-practices","text":"Be sure to use best practices up for writing Dockerfiles in order to reduce the container image size.","title":"Best practices"},{"location":"our-helm-charts/common-library-add-ons/","text":"Common library add-ons \u00b6 Our Helm charts have a few add-ons which are meant to simplify some features you might be looking for. These are sidecars that run in the same pod as your application you configured it with. Code Server \u00b6 The code-server add-on can be used to access and modify persistent volume data in your application. This can be useful when you need to edit the persistent volume data, for example with Home Assistant. Example values \u00b6 Below is a snippet from a values.yaml using the add-on. More configuration options can be found in our common chart documentation. Note This example will mount /config into the code-server sidecar. addons : codeserver : enabled : true image : repository : codercom/code-server tag : 3.9.0 workingDir : \"/config\" args : - --auth - \"none\" - --user-data-dir - \"/config/.vscode\" - --extensions-dir - \"/config/.vscode\" ingress : enabled : true annotations : kubernetes.io/ingress.class : \"nginx\" hosts : - host : app-config.domain.tld paths : - path : / pathType : Prefix tls : - hosts : - app-config.domain.tld volumeMounts : - name : config mountPath : /config Wireguard VPN \u00b6 The Wireguard add-on enables you to force all (or selected) network traffic through a VPN. This example shows how to add a Wireguard sidecar to our qBittorrent Helm chart . It does not cover all of the configuration possibilities of the Wireguard client image , but should give a good starting point for configuring a similar setup. Example values \u00b6 Below is an annotated example values.yaml that will result in a qBittorrent container with all its traffic routed through a VPN. In order to have functioning ingress and/or probes, it might be required to open certain networks or ports on the VPN firewall. That is beyond the scope of this document. Please refer to the Wireguard client image for more details on these environment variables. Note The WAIT_FOR_VPN environment variable is specifically implemented by our own qBittorrent image, and it will not work with other container images. image : repository : k8sathome/qbittorrent tag : v4.3.3 pullPolicy : IfNotPresent env : # Our qBittorrent image has a feature that can wait for the VPN to be connected before actually starting the application. # It does this by checking the contents of a file /shared/vpnstatus to contain the string 'connected'. WAIT_FOR_VPN : \"true\" persistence : config : enabled : true type : emptyDir mountPath : /config # This should be enabled so that both the qBittorrent and Wireguard container have access to a shared volume mounted to /shared. # It will be used to communicate between the two containers. shared : enabled : true type : emptyDir mountPath : /shared addons : vpn : enabled : true # This Should be set to `wireguard`. This will set the add-on to use the default settings for Wireguard based connections. type : wireguard # If the podSecurityContext is set to run as a different user, make sure to run the Wireguard container as UID/GID 568. # This is required for it to be able to read certain configuration files. securityContext : runAsUser : 568 runAsGroup : 568 env : # Enable a killswitch that kills all trafic when the VPN is not connected KILLSWITCH : \"true\" # The wireguard configuration file provided by your VPN provider goes here. # # Set AllowedIPs to 0.0.0.0/0 to route all traffic through the VPN. # # Pay close attention to the PostUp and PreDown lines. They must be added if you wish to run a script when the connection # is opened / closed. configFile : |- [Interface] PrivateKey = <my-private-key> Address = <interface address> DNS = <interface DNS server> PostUp = /config/up.sh %i PreDown = /config/down.sh %i [Peer] PublicKey = <my-public-key> AllowedIPs = 0.0.0.0/0 Endpoint = <peer endpoint> # The scripts that get run when the VPN connection opens/closes are defined here. # The default scripts will write a string to represent the current connection state to a file. # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application. scripts : up : |- #!/bin/bash echo \"connected\" > /shared/vpnstatus down : |- #!/bin/bash echo \"disconnected\" > /shared/vpnstatus OpenVPN \u00b6 Similar to the Wireguard VPN, the OpenVPN add-on enables you to force all (or selected) network traffic through a VPN. This example shows how to add an OpenVPN sidecar to our qBittorrent Helm chart . It does not cover all of the configuration possibilities of the OpenVPN client image by @dperson , but should give a good starting point for configuring a similar setup. Example values \u00b6 Below is an annotated example values.yaml that will result in a qBittorrent container with all its traffic routed through a VPN. In order to have functioning ingress and/or probes, it might be required to open certain networks or ports on the VPN firewall. That is beyond the scope of this document. Please refer to the OpenVPN client image for more details on these environment variables. Note The WAIT_FOR_VPN environment variable is specifically implemented by our own qBittorrent image, and it will not work with other container images. image : repository : k8sathome/qbittorrent tag : v4.3.3 pullPolicy : IfNotPresent env : # Our qBittorrent image has a feature that can wait for the VPN to be # connected before actually starting the application. # It does this by checking the contents of a file /shared/vpnstatus to # contain the string 'connected'. WAIT_FOR_VPN : \"true\" persistence : config : enabled : true type : emptyDir mountPath : /config # This should be enabled so that both the qBittorrent and OpenVPN container have access to a shared volume mounted to /shared. # It will be used to communicate between the two containers. shared : enabled : true type : emptyDir mountPath : /shared addons : vpn : enabled : true # This Should be set to `openvpn`. This will set the add-on to use the default settings for OpenVPN based connections. type : openvpn openvpn : # This gets read by the Helm chart. The default OpenVPN image reads this and uses it to connect to the VPN provider. auth : | myuser mypassword # If the podSecurityContext is set to run as a different user, make sure to run the OpenVPN container as root. # This is required for it to be able to read certain configuration files. securityContext : runAsGroup : 0 runAsUser : 0 env : # Set this environment variable to 'on' to make sure all traffic gets routed through the VPN container. # Make sure to check the other environment variables for the OpenVPN image to see how you can exclude certain # traffic from these firewall rules. FIREWALL : 'on' # The .ovpn file provided by your VPN provider goes here. # # Any CA / certificate must either be placed inline, or provided through an additionalVolumeMount so that OpenVPN can find it. # # Pay close attention to the last 3 lines in this file. They must be added if you wish to run a script when the connection # is opened / closed. configFile : |- client dev tun proto udp remote my-awesome-vpn-provider.com 995 remote-cert-tls server resolv-retry infinite nobind tls-version-min 1.2 cipher AES-128-GCM compress ncp-disable tun-mtu-extra 32 auth-user-pass <ca> -----BEGIN CERTIFICATE----- MIIDMTCCAhmgAwIBAgIJAKnGGJK6qLqSMA0GCSqGSIb3DQEBCwUAMBQxEjAQBgNV -----END CERTIFICATE----- </ca> script-security 2 up /vpn/up.sh down /vpn/down.sh # The scripts that get run when the VPN connection opens/closes are defined here. # The default scripts will write a string to represent the current connection state to a file. # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application. scripts : up : |- #!/bin/bash /etc/openvpn/up.sh echo \"connected\" > /shared/vpnstatus down : |- #!/bin/bash /etc/openvpn/down.sh echo \"disconnected\" > /shared/vpnstatus","title":"Common Add-ons"},{"location":"our-helm-charts/common-library-add-ons/#common-library-add-ons","text":"Our Helm charts have a few add-ons which are meant to simplify some features you might be looking for. These are sidecars that run in the same pod as your application you configured it with.","title":"Common library add-ons"},{"location":"our-helm-charts/common-library-add-ons/#code-server","text":"The code-server add-on can be used to access and modify persistent volume data in your application. This can be useful when you need to edit the persistent volume data, for example with Home Assistant.","title":"Code Server"},{"location":"our-helm-charts/common-library-add-ons/#example-values","text":"Below is a snippet from a values.yaml using the add-on. More configuration options can be found in our common chart documentation. Note This example will mount /config into the code-server sidecar. addons : codeserver : enabled : true image : repository : codercom/code-server tag : 3.9.0 workingDir : \"/config\" args : - --auth - \"none\" - --user-data-dir - \"/config/.vscode\" - --extensions-dir - \"/config/.vscode\" ingress : enabled : true annotations : kubernetes.io/ingress.class : \"nginx\" hosts : - host : app-config.domain.tld paths : - path : / pathType : Prefix tls : - hosts : - app-config.domain.tld volumeMounts : - name : config mountPath : /config","title":"Example values"},{"location":"our-helm-charts/common-library-add-ons/#wireguard-vpn","text":"The Wireguard add-on enables you to force all (or selected) network traffic through a VPN. This example shows how to add a Wireguard sidecar to our qBittorrent Helm chart . It does not cover all of the configuration possibilities of the Wireguard client image , but should give a good starting point for configuring a similar setup.","title":"Wireguard VPN"},{"location":"our-helm-charts/common-library-add-ons/#example-values_1","text":"Below is an annotated example values.yaml that will result in a qBittorrent container with all its traffic routed through a VPN. In order to have functioning ingress and/or probes, it might be required to open certain networks or ports on the VPN firewall. That is beyond the scope of this document. Please refer to the Wireguard client image for more details on these environment variables. Note The WAIT_FOR_VPN environment variable is specifically implemented by our own qBittorrent image, and it will not work with other container images. image : repository : k8sathome/qbittorrent tag : v4.3.3 pullPolicy : IfNotPresent env : # Our qBittorrent image has a feature that can wait for the VPN to be connected before actually starting the application. # It does this by checking the contents of a file /shared/vpnstatus to contain the string 'connected'. WAIT_FOR_VPN : \"true\" persistence : config : enabled : true type : emptyDir mountPath : /config # This should be enabled so that both the qBittorrent and Wireguard container have access to a shared volume mounted to /shared. # It will be used to communicate between the two containers. shared : enabled : true type : emptyDir mountPath : /shared addons : vpn : enabled : true # This Should be set to `wireguard`. This will set the add-on to use the default settings for Wireguard based connections. type : wireguard # If the podSecurityContext is set to run as a different user, make sure to run the Wireguard container as UID/GID 568. # This is required for it to be able to read certain configuration files. securityContext : runAsUser : 568 runAsGroup : 568 env : # Enable a killswitch that kills all trafic when the VPN is not connected KILLSWITCH : \"true\" # The wireguard configuration file provided by your VPN provider goes here. # # Set AllowedIPs to 0.0.0.0/0 to route all traffic through the VPN. # # Pay close attention to the PostUp and PreDown lines. They must be added if you wish to run a script when the connection # is opened / closed. configFile : |- [Interface] PrivateKey = <my-private-key> Address = <interface address> DNS = <interface DNS server> PostUp = /config/up.sh %i PreDown = /config/down.sh %i [Peer] PublicKey = <my-public-key> AllowedIPs = 0.0.0.0/0 Endpoint = <peer endpoint> # The scripts that get run when the VPN connection opens/closes are defined here. # The default scripts will write a string to represent the current connection state to a file. # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application. scripts : up : |- #!/bin/bash echo \"connected\" > /shared/vpnstatus down : |- #!/bin/bash echo \"disconnected\" > /shared/vpnstatus","title":"Example values"},{"location":"our-helm-charts/common-library-add-ons/#openvpn","text":"Similar to the Wireguard VPN, the OpenVPN add-on enables you to force all (or selected) network traffic through a VPN. This example shows how to add an OpenVPN sidecar to our qBittorrent Helm chart . It does not cover all of the configuration possibilities of the OpenVPN client image by @dperson , but should give a good starting point for configuring a similar setup.","title":"OpenVPN"},{"location":"our-helm-charts/common-library-add-ons/#example-values_2","text":"Below is an annotated example values.yaml that will result in a qBittorrent container with all its traffic routed through a VPN. In order to have functioning ingress and/or probes, it might be required to open certain networks or ports on the VPN firewall. That is beyond the scope of this document. Please refer to the OpenVPN client image for more details on these environment variables. Note The WAIT_FOR_VPN environment variable is specifically implemented by our own qBittorrent image, and it will not work with other container images. image : repository : k8sathome/qbittorrent tag : v4.3.3 pullPolicy : IfNotPresent env : # Our qBittorrent image has a feature that can wait for the VPN to be # connected before actually starting the application. # It does this by checking the contents of a file /shared/vpnstatus to # contain the string 'connected'. WAIT_FOR_VPN : \"true\" persistence : config : enabled : true type : emptyDir mountPath : /config # This should be enabled so that both the qBittorrent and OpenVPN container have access to a shared volume mounted to /shared. # It will be used to communicate between the two containers. shared : enabled : true type : emptyDir mountPath : /shared addons : vpn : enabled : true # This Should be set to `openvpn`. This will set the add-on to use the default settings for OpenVPN based connections. type : openvpn openvpn : # This gets read by the Helm chart. The default OpenVPN image reads this and uses it to connect to the VPN provider. auth : | myuser mypassword # If the podSecurityContext is set to run as a different user, make sure to run the OpenVPN container as root. # This is required for it to be able to read certain configuration files. securityContext : runAsGroup : 0 runAsUser : 0 env : # Set this environment variable to 'on' to make sure all traffic gets routed through the VPN container. # Make sure to check the other environment variables for the OpenVPN image to see how you can exclude certain # traffic from these firewall rules. FIREWALL : 'on' # The .ovpn file provided by your VPN provider goes here. # # Any CA / certificate must either be placed inline, or provided through an additionalVolumeMount so that OpenVPN can find it. # # Pay close attention to the last 3 lines in this file. They must be added if you wish to run a script when the connection # is opened / closed. configFile : |- client dev tun proto udp remote my-awesome-vpn-provider.com 995 remote-cert-tls server resolv-retry infinite nobind tls-version-min 1.2 cipher AES-128-GCM compress ncp-disable tun-mtu-extra 32 auth-user-pass <ca> -----BEGIN CERTIFICATE----- MIIDMTCCAhmgAwIBAgIJAKnGGJK6qLqSMA0GCSqGSIb3DQEBCwUAMBQxEjAQBgNV -----END CERTIFICATE----- </ca> script-security 2 up /vpn/up.sh down /vpn/down.sh # The scripts that get run when the VPN connection opens/closes are defined here. # The default scripts will write a string to represent the current connection state to a file. # Our qBittorrent image has a feature that can wait for this file to contain the word 'connected' before actually starting the application. scripts : up : |- #!/bin/bash /etc/openvpn/up.sh echo \"connected\" > /shared/vpnstatus down : |- #!/bin/bash /etc/openvpn/down.sh echo \"disconnected\" > /shared/vpnstatus","title":"Example values"},{"location":"our-helm-charts/common-library-storage/","text":"Common library Storage \u00b6 Most of our application Helm charts consume our Common library Helm chart. This page describes the different ways you can attach storage to these charts. Types \u00b6 These are the types of storage that we support in our common library. Of course, other types are possible with the custom type. Persistent Volume Claim \u00b6 This is probably the most common storage type, therefore it is also the default when no type is specified. It can be attached in two ways. Dynamically provisioned \u00b6 Our charts can be configured to create the required persistentVolumeClaim manifests on the fly. Field Mandatory Docs / Description enabled Yes type Yes accessMode Yes link size Yes link mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. storageClass No Storage class to use for this volume. retain No Set to true to retain the PVC upon helm uninstall . Minimal config: persistence : config : enabled : true type : pvc accessMode : ReadWriteOnce size : 1Gi This will create a 1Gi RWO PVC named RELEASE-NAME-config with the default storageClass, which will mount to /config . Existing claim \u00b6 Our charts can be configured to attach to a pre-existing persistentVolumeClaim. Field Mandatory Docs / Description enabled Yes type Yes existingClaim Yes Name of the existing PVC mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . subPath No Specifies a sub-path inside the referenced volume instead of its root. readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : pvc existingClaim : myAppData This will mount an existing PVC named myAppData to /config . Empty Dir \u00b6 Sometimes you need to share some data between containers, or need some scratch space. That is where an emptyDir can come in. See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes medium No Set this to Memory to mount a tmpfs (RAM-backed filesystem) instead of the storage medium that backs the node. sizeLimit No If the SizeMemoryBackedVolumes feature gate is enabled, you can specify a size for memory backed volumes. mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : emptyDir This will create an ephemeral emptyDir volume and mount it to /config . Host path \u00b6 In order to mount a path from the node where the Pod is running you can use a hostPath type persistence item. This can also be used to mount an attached USB device to a Pod. Note that this will most likely also require setting an elevated securityContext . See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes hostPath Yes Which path on the host should be mounted. hostPathType No Specifying a hostPathType adds a check before trying to mount the path. See Kubernetes documentation for options. mountPath No Where to mount the volume in the main container. Defaults to the value of hostPath . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : hostPath hostPath : /dev This will mount the /dev folder from the underlying host to /dev in the container. configMap \u00b6 In order to mount a configMap to a mount point within the Pod you can use the configMap type persistence item. Field Mandatory Docs / Description enabled Yes type Yes name Yes Which configMap should be mounted. Supports Helm templating. defaultMode No The default file access permission bit. items No Specify item-specific configuration. Will be passed 1:1 to the volumeSpec. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the configMap will be read-only. Minimal config: persistence : config : enabled : true type : configMap name : mySettings This will mount the contents of the pre-existing mySettings configMap to /config . Secret \u00b6 In order to mount a Secret to a mount point within the Pod you can use the secret type persistence item. Field Mandatory Docs / Description enabled Yes type Yes name Yes Which Secret should be mounted. Supports Helm templating. defaultMode No The default file access permission bit. items No Specify item-specific configuration. Will be passed 1:1 to the volumeSpec. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the Secret will be read-only. Minimal config: persistence : config : enabled : true type : secret name : mySecret This will mount the contents of the pre-existing mySecret Secret to /config . NFS Volume \u00b6 To mount an NFS share to your Pod you can either pre-create a persistentVolumeClaim referring to it, or you can specify an inline NFS volume: Note Mounting an NFS share this way does not allow for specifying mount options. If you require these, you must create a PVC to mount the share. Field Mandatory Docs / Description enabled Yes type Yes server Yes Host name or IP address of the NFS server. path Yes The path on the server to mount. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the Secret will be read-only. Minimal config: persistence : config : enabled : true type : nfs server : 10.10.0.8 path : /tank/nas/library This will mount the NFS share /tank/nas/library on server 10.10.0.8 to /config . Custom \u00b6 When you wish to specify a custom volume, you can use the custom type. This can be used for example to mount configMap or Secret objects. See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes volumeSpec Yes Define the raw Volume spec here. mountPath No Where to mount the volume in the main container. Defaults to the value of hostPath . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. Permissions \u00b6 Our charts do not modify file or folder permissions on volumes out of the box. This means that you will have to make sure that your storage can be written to by the application. See Permissions for more details about your options. Multiple subPaths for 1 volume \u00b6 It is possible to mount multiple subPaths from the same volume to the main container. This can be achieved by specifying subPath with a list instead of a string. Note It is not possible to define mountPath at the top level when using this feature Examples: persistence : config : enabled : true type : custom volumeSpec : configMap : name : myData subPath : - path : myFirstScript.sh mountPath : /data/myFirstScript.sh - path : myCertificate.pem mountPath : /certs/myCertificate.pem readOnly : true persistence : config : enabled : true type : pvc existingClaim : myAppData subPath : - path : . mountPath : /my_media - path : Series mountPath : /series - path : Downloads mountPath : /downloads","title":"Common Storage"},{"location":"our-helm-charts/common-library-storage/#common-library-storage","text":"Most of our application Helm charts consume our Common library Helm chart. This page describes the different ways you can attach storage to these charts.","title":"Common library Storage"},{"location":"our-helm-charts/common-library-storage/#types","text":"These are the types of storage that we support in our common library. Of course, other types are possible with the custom type.","title":"Types"},{"location":"our-helm-charts/common-library-storage/#persistent-volume-claim","text":"This is probably the most common storage type, therefore it is also the default when no type is specified. It can be attached in two ways.","title":"Persistent Volume Claim"},{"location":"our-helm-charts/common-library-storage/#dynamically-provisioned","text":"Our charts can be configured to create the required persistentVolumeClaim manifests on the fly. Field Mandatory Docs / Description enabled Yes type Yes accessMode Yes link size Yes link mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. storageClass No Storage class to use for this volume. retain No Set to true to retain the PVC upon helm uninstall . Minimal config: persistence : config : enabled : true type : pvc accessMode : ReadWriteOnce size : 1Gi This will create a 1Gi RWO PVC named RELEASE-NAME-config with the default storageClass, which will mount to /config .","title":"Dynamically provisioned"},{"location":"our-helm-charts/common-library-storage/#existing-claim","text":"Our charts can be configured to attach to a pre-existing persistentVolumeClaim. Field Mandatory Docs / Description enabled Yes type Yes existingClaim Yes Name of the existing PVC mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . subPath No Specifies a sub-path inside the referenced volume instead of its root. readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : pvc existingClaim : myAppData This will mount an existing PVC named myAppData to /config .","title":"Existing claim"},{"location":"our-helm-charts/common-library-storage/#empty-dir","text":"Sometimes you need to share some data between containers, or need some scratch space. That is where an emptyDir can come in. See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes medium No Set this to Memory to mount a tmpfs (RAM-backed filesystem) instead of the storage medium that backs the node. sizeLimit No If the SizeMemoryBackedVolumes feature gate is enabled, you can specify a size for memory backed volumes. mountPath No Where to mount the volume in the main container. Defaults to /<name_of_the_volume> . nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : emptyDir This will create an ephemeral emptyDir volume and mount it to /config .","title":"Empty Dir"},{"location":"our-helm-charts/common-library-storage/#host-path","text":"In order to mount a path from the node where the Pod is running you can use a hostPath type persistence item. This can also be used to mount an attached USB device to a Pod. Note that this will most likely also require setting an elevated securityContext . See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes hostPath Yes Which path on the host should be mounted. hostPathType No Specifying a hostPathType adds a check before trying to mount the path. See Kubernetes documentation for options. mountPath No Where to mount the volume in the main container. Defaults to the value of hostPath . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume. Minimal config: persistence : config : enabled : true type : hostPath hostPath : /dev This will mount the /dev folder from the underlying host to /dev in the container.","title":"Host path"},{"location":"our-helm-charts/common-library-storage/#configmap","text":"In order to mount a configMap to a mount point within the Pod you can use the configMap type persistence item. Field Mandatory Docs / Description enabled Yes type Yes name Yes Which configMap should be mounted. Supports Helm templating. defaultMode No The default file access permission bit. items No Specify item-specific configuration. Will be passed 1:1 to the volumeSpec. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the configMap will be read-only. Minimal config: persistence : config : enabled : true type : configMap name : mySettings This will mount the contents of the pre-existing mySettings configMap to /config .","title":"configMap"},{"location":"our-helm-charts/common-library-storage/#secret","text":"In order to mount a Secret to a mount point within the Pod you can use the secret type persistence item. Field Mandatory Docs / Description enabled Yes type Yes name Yes Which Secret should be mounted. Supports Helm templating. defaultMode No The default file access permission bit. items No Specify item-specific configuration. Will be passed 1:1 to the volumeSpec. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the Secret will be read-only. Minimal config: persistence : config : enabled : true type : secret name : mySecret This will mount the contents of the pre-existing mySecret Secret to /config .","title":"Secret"},{"location":"our-helm-charts/common-library-storage/#nfs-volume","text":"To mount an NFS share to your Pod you can either pre-create a persistentVolumeClaim referring to it, or you can specify an inline NFS volume: Note Mounting an NFS share this way does not allow for specifying mount options. If you require these, you must create a PVC to mount the share. Field Mandatory Docs / Description enabled Yes type Yes server Yes Host name or IP address of the NFS server. path Yes The path on the server to mount. readOnly No Explicitly specify if the volume should be mounted read-only. Even if not specified, the Secret will be read-only. Minimal config: persistence : config : enabled : true type : nfs server : 10.10.0.8 path : /tank/nas/library This will mount the NFS share /tank/nas/library on server 10.10.0.8 to /config .","title":"NFS Volume"},{"location":"our-helm-charts/common-library-storage/#custom","text":"When you wish to specify a custom volume, you can use the custom type. This can be used for example to mount configMap or Secret objects. See the Kubernetes docs for more information. Field Mandatory Docs / Description enabled Yes type Yes volumeSpec Yes Define the raw Volume spec here. mountPath No Where to mount the volume in the main container. Defaults to the value of hostPath . readOnly No Specify if the volume should be mounted read-only. nameOverride No Override the name suffix that is used for this volume.","title":"Custom"},{"location":"our-helm-charts/common-library-storage/#permissions","text":"Our charts do not modify file or folder permissions on volumes out of the box. This means that you will have to make sure that your storage can be written to by the application. See Permissions for more details about your options.","title":"Permissions"},{"location":"our-helm-charts/common-library-storage/#multiple-subpaths-for-1-volume","text":"It is possible to mount multiple subPaths from the same volume to the main container. This can be achieved by specifying subPath with a list instead of a string. Note It is not possible to define mountPath at the top level when using this feature Examples: persistence : config : enabled : true type : custom volumeSpec : configMap : name : myData subPath : - path : myFirstScript.sh mountPath : /data/myFirstScript.sh - path : myCertificate.pem mountPath : /certs/myCertificate.pem readOnly : true persistence : config : enabled : true type : pvc existingClaim : myAppData subPath : - path : . mountPath : /my_media - path : Series mountPath : /series - path : Downloads mountPath : /downloads","title":"Multiple subPaths for 1 volume"},{"location":"our-helm-charts/common-library/","text":"Common library \u00b6 Most of our application Helm charts consume our Common library Helm chart. Note The Common library chart is not meant to be installed directly, application charts use the Common library as a dependency. Background \u00b6 In Helm 3, their team introduced the concept of a Library chart . A library chart is a type of Helm chart that defines chart primitives or definitions which can be shared by Helm templates in other charts. This allows users to share snippets of code that can be re-used across charts, avoiding repetition and keeping charts DRY. The common library was created because we saw many charts requiring only a few select configuration options in their Helm charts. Note Take one of the many charts like sonarr or nzbget . Each of these charts only require setting service , port , persistence , ingress and image since state and app configuration is handled by the application itself. In order to stay somewhat DRY (Don't Repeat Yourself) and keeping with Helm 3 usage for a Library chart, we saw this pattern and decided it was worth it for us to create a library. This means each one of these app charts has a dependency on what we call the common library. Changelog \u00b6 To view the changelog for the common library see here . Source code \u00b6 The source code for our library chart can be found here .","title":"Common"},{"location":"our-helm-charts/common-library/#common-library","text":"Most of our application Helm charts consume our Common library Helm chart. Note The Common library chart is not meant to be installed directly, application charts use the Common library as a dependency.","title":"Common library"},{"location":"our-helm-charts/common-library/#background","text":"In Helm 3, their team introduced the concept of a Library chart . A library chart is a type of Helm chart that defines chart primitives or definitions which can be shared by Helm templates in other charts. This allows users to share snippets of code that can be re-used across charts, avoiding repetition and keeping charts DRY. The common library was created because we saw many charts requiring only a few select configuration options in their Helm charts. Note Take one of the many charts like sonarr or nzbget . Each of these charts only require setting service , port , persistence , ingress and image since state and app configuration is handled by the application itself. In order to stay somewhat DRY (Don't Repeat Yourself) and keeping with Helm 3 usage for a Library chart, we saw this pattern and decided it was worth it for us to create a library. This means each one of these app charts has a dependency on what we call the common library.","title":"Background"},{"location":"our-helm-charts/common-library/#changelog","text":"To view the changelog for the common library see here .","title":"Changelog"},{"location":"our-helm-charts/common-library/#source-code","text":"The source code for our library chart can be found here .","title":"Source code"},{"location":"our-helm-charts/introduction/","text":"Introduction \u00b6 Helm must be installed to use our charts. Refer to Helm's documentation to get started. Installation \u00b6 helm repo add k8s-at-spiti https://k8s-at-spiti.com/charts/ You can then run helm search repo k8s-at-spiti to see the charts. Charts \u00b6 See Artifact Hub for a complete list.","title":"Introduction"},{"location":"our-helm-charts/introduction/#introduction","text":"Helm must be installed to use our charts. Refer to Helm's documentation to get started.","title":"Introduction"},{"location":"our-helm-charts/introduction/#installation","text":"helm repo add k8s-at-spiti https://k8s-at-spiti.com/charts/ You can then run helm search repo k8s-at-spiti to see the charts.","title":"Installation"},{"location":"our-helm-charts/introduction/#charts","text":"See Artifact Hub for a complete list.","title":"Charts"},{"location":"our-helm-charts/development/creating-a-new-chart/","text":"Creating a new chart \u00b6 Dependencies \u00b6 If you would like to help create new charts using the common library, there's a few tools you will need. helm Task (optional) pre-commmit (optional - required with tasks) Prerequisites \u00b6 If you wish to use the task commands to create a new chart from the template, you need to make sure that Task is installed. Please see the project docs for more information on how to install it. Creating a new chart \u00b6 To create a new chart, run the following: git clone https://github.com/k8s-at-spiti/charts.git cd charts task deps:install task chart:create CHART_TYPE = stable CHART = chart-name Second, be sure to checkout the many charts that already use the common library like qBittorrent , node-red or the many others in this repository. You can recognize which charts include this common chart as dependency by the following snippet: # Chart.yaml ... dependencies : - name : common version : 4.3.0 # make sure to use the latest common library version available repository : https://library-charts.k8s-at-spiti.com ... Chart metadata \u00b6 In order to keep our chart documentation similar across all charts we rely heavily on template files. We use helm-docs to render these templates into the README.md file. Don't forgot to populate/update the chart information in charts/stable/chart-name/Chart.yaml and charts/stable/chart-name/values.yaml files. Aside from the default chart metadata (such as the version , appVersion , etc fields), we require a artifacthub.io/changes chart annotation (as documented by ArtifactHUB ) that describes the modifications in this chart version. # Chart.yaml ... annotations : artifacthub.io/changes : | - kind: added description: Initial version ... Info Because we rely so heavily on standardized templates, any manual changes to README.md will get overwriten when the documentation is generated. Any chart-specific documentation should go in the README_CONFIG.md.gotmpl file. Testing \u00b6 When testing locally, make sure you update the dependencies from within the chart directory: helm dependency update If making local changes to the common library, the test chart may reference the local development chart: # common-test/Chart.yaml ... dependencies : - name : common version : <new version> repository : file://.../common ... Be sure to lint your chart to check for any errors. # Linting task chart:lint CHART_TYPE = stable CHART = chart-name task chart:ct-lint CHART_TYPE = stable CHART = chart-name Values \u00b6 Edit values.yaml with some basic defaults you want to present to the user. Please ensure you anotate them with helm-docs e.g. # # IMPORTANT NOTE # # This chart inherits from our common library chart. You can check the default values/options here: # https://github.com/k8s-at-spiti/library-charts/tree/master/charts/stable/common/values.yaml # image : # -- image repository repository : nodered/node-red # -- image tag tag : 1.2.5 # -- image pull policy pullPolicy : IfNotPresent # -- environment variables. # @default -- See below env : # -- Set the container timezone TZ : UTC # -- Configures service settings for the chart. # @default -- See values.yaml service : main : ports : http : port : 1880 ingress : # -- Enable and configure ingress settings for the chart under this key. # @default -- See values.yaml main : enabled : false persistence : data : enabled : false type : emptyDir mountPath : /data If not using a service, set the service.enabled to false . ... service : main : enabled : false ... Templates \u00b6 Basic \u00b6 In its most basic form a new chart can consist of two simple files in the templates folder. This will automatically render everything, based only on what is (or isn't) present in values.yaml . templates/common.yaml : {{ include \"common.all . }} templates/NOTES.txt : {{ include \"common.notes.defaultNotes\" . }} Advanced \u00b6 Sometimes it is not required to implement additional logic in a chart that you do not wish to expose through settings in values.yaml . For example, when you want to always mount a Secret or configMap as a volume in the Pod. In that case it is also possible to write more advanced template files. templates/common.yaml : {{ /* First Make sure all variables are set and merged properly */ }} {{ - include \"common.values.setup\" . }} {{ /* Append the configMap volume to the volumes */ }} {{ - define \"myapp.settingsVolume\" - }} enabled : \"true\" mountPath : \"/app/configuration.yaml\" subPath : \"configuration.yaml\" type : \"custom\" volumeSpec : configMap : name : {{ include \"common.names.fullname\" . }} -settings {{ - end - }} {{ - $_ : = set .Values.persistence \"myapp-settings\" (include \"myapp.settingsVolume\" . | fromYaml) - }} {{ /* Render the templates */ }} {{ include \"common.all\" . }} An actual example of this can be found in the zigbee2mqtt chart.","title":"Creating a new chart"},{"location":"our-helm-charts/development/creating-a-new-chart/#creating-a-new-chart","text":"","title":"Creating a new chart"},{"location":"our-helm-charts/development/creating-a-new-chart/#dependencies","text":"If you would like to help create new charts using the common library, there's a few tools you will need. helm Task (optional) pre-commmit (optional - required with tasks)","title":"Dependencies"},{"location":"our-helm-charts/development/creating-a-new-chart/#prerequisites","text":"If you wish to use the task commands to create a new chart from the template, you need to make sure that Task is installed. Please see the project docs for more information on how to install it.","title":"Prerequisites"},{"location":"our-helm-charts/development/creating-a-new-chart/#creating-a-new-chart_1","text":"To create a new chart, run the following: git clone https://github.com/k8s-at-spiti/charts.git cd charts task deps:install task chart:create CHART_TYPE = stable CHART = chart-name Second, be sure to checkout the many charts that already use the common library like qBittorrent , node-red or the many others in this repository. You can recognize which charts include this common chart as dependency by the following snippet: # Chart.yaml ... dependencies : - name : common version : 4.3.0 # make sure to use the latest common library version available repository : https://library-charts.k8s-at-spiti.com ...","title":"Creating a new chart"},{"location":"our-helm-charts/development/creating-a-new-chart/#chart-metadata","text":"In order to keep our chart documentation similar across all charts we rely heavily on template files. We use helm-docs to render these templates into the README.md file. Don't forgot to populate/update the chart information in charts/stable/chart-name/Chart.yaml and charts/stable/chart-name/values.yaml files. Aside from the default chart metadata (such as the version , appVersion , etc fields), we require a artifacthub.io/changes chart annotation (as documented by ArtifactHUB ) that describes the modifications in this chart version. # Chart.yaml ... annotations : artifacthub.io/changes : | - kind: added description: Initial version ... Info Because we rely so heavily on standardized templates, any manual changes to README.md will get overwriten when the documentation is generated. Any chart-specific documentation should go in the README_CONFIG.md.gotmpl file.","title":"Chart metadata"},{"location":"our-helm-charts/development/creating-a-new-chart/#testing","text":"When testing locally, make sure you update the dependencies from within the chart directory: helm dependency update If making local changes to the common library, the test chart may reference the local development chart: # common-test/Chart.yaml ... dependencies : - name : common version : <new version> repository : file://.../common ... Be sure to lint your chart to check for any errors. # Linting task chart:lint CHART_TYPE = stable CHART = chart-name task chart:ct-lint CHART_TYPE = stable CHART = chart-name","title":"Testing"},{"location":"our-helm-charts/development/creating-a-new-chart/#values","text":"Edit values.yaml with some basic defaults you want to present to the user. Please ensure you anotate them with helm-docs e.g. # # IMPORTANT NOTE # # This chart inherits from our common library chart. You can check the default values/options here: # https://github.com/k8s-at-spiti/library-charts/tree/master/charts/stable/common/values.yaml # image : # -- image repository repository : nodered/node-red # -- image tag tag : 1.2.5 # -- image pull policy pullPolicy : IfNotPresent # -- environment variables. # @default -- See below env : # -- Set the container timezone TZ : UTC # -- Configures service settings for the chart. # @default -- See values.yaml service : main : ports : http : port : 1880 ingress : # -- Enable and configure ingress settings for the chart under this key. # @default -- See values.yaml main : enabled : false persistence : data : enabled : false type : emptyDir mountPath : /data If not using a service, set the service.enabled to false . ... service : main : enabled : false ...","title":"Values"},{"location":"our-helm-charts/development/creating-a-new-chart/#templates","text":"","title":"Templates"},{"location":"our-helm-charts/development/creating-a-new-chart/#basic","text":"In its most basic form a new chart can consist of two simple files in the templates folder. This will automatically render everything, based only on what is (or isn't) present in values.yaml . templates/common.yaml : {{ include \"common.all . }} templates/NOTES.txt : {{ include \"common.notes.defaultNotes\" . }}","title":"Basic"},{"location":"our-helm-charts/development/creating-a-new-chart/#advanced","text":"Sometimes it is not required to implement additional logic in a chart that you do not wish to expose through settings in values.yaml . For example, when you want to always mount a Secret or configMap as a volume in the Pod. In that case it is also possible to write more advanced template files. templates/common.yaml : {{ /* First Make sure all variables are set and merged properly */ }} {{ - include \"common.values.setup\" . }} {{ /* Append the configMap volume to the volumes */ }} {{ - define \"myapp.settingsVolume\" - }} enabled : \"true\" mountPath : \"/app/configuration.yaml\" subPath : \"configuration.yaml\" type : \"custom\" volumeSpec : configMap : name : {{ include \"common.names.fullname\" . }} -settings {{ - end - }} {{ - $_ : = set .Values.persistence \"myapp-settings\" (include \"myapp.settingsVolume\" . | fromYaml) - }} {{ /* Render the templates */ }} {{ include \"common.all\" . }} An actual example of this can be found in the zigbee2mqtt chart.","title":"Advanced"},{"location":"our-helm-charts/development/databases/","text":"Databases \u00b6 Databases from other repositories can be added to charts as dependencies. The databases are only installed if the <db name>.enabled key is set to true. See home-assistant for reference. Chart.yaml \u00b6 Add the following entries to Chart.yaml under the dependencies section. ... dependencies : - name : postgresql version : <chart version> repository : https://charts.bitnami.com/bitnami condition : postgresql.enabled - name : mariadb version : <chart version> repository : https://charts.bitnami.com/bitnami condition : mariadb.enabled - name : influxdb version : <chart version> repository : https://charts.bitnami.com/bitnami condition : influxdb.enabled values.yaml \u00b6 Update the values.yaml with the following. Refer the respective database chart values.yaml for additional values. MariaDB \u00b6 ... mariadb : enabled : false architecture : standalone auth : database : <chart name> username : <chart name> password : <chart password> rootPassword : home-assistantrootpass primary : persistence : enabled : false Postgres \u00b6 ... postgresql : enabled : false postgresqlUsername : <chart name> postgresqlPassword : <chart password> postgresqlDatabase : <chart name> persistence : enabled : false InfluxDB \u00b6 ... influxdb : enabled : false architecture : standalone database : <chart name> authEnabled : false persistence : enabled : false","title":"Databases"},{"location":"our-helm-charts/development/databases/#databases","text":"Databases from other repositories can be added to charts as dependencies. The databases are only installed if the <db name>.enabled key is set to true. See home-assistant for reference.","title":"Databases"},{"location":"our-helm-charts/development/databases/#chartyaml","text":"Add the following entries to Chart.yaml under the dependencies section. ... dependencies : - name : postgresql version : <chart version> repository : https://charts.bitnami.com/bitnami condition : postgresql.enabled - name : mariadb version : <chart version> repository : https://charts.bitnami.com/bitnami condition : mariadb.enabled - name : influxdb version : <chart version> repository : https://charts.bitnami.com/bitnami condition : influxdb.enabled","title":"Chart.yaml"},{"location":"our-helm-charts/development/databases/#valuesyaml","text":"Update the values.yaml with the following. Refer the respective database chart values.yaml for additional values.","title":"values.yaml"},{"location":"our-helm-charts/development/databases/#mariadb","text":"... mariadb : enabled : false architecture : standalone auth : database : <chart name> username : <chart name> password : <chart password> rootPassword : home-assistantrootpass primary : persistence : enabled : false","title":"MariaDB"},{"location":"our-helm-charts/development/databases/#postgres","text":"... postgresql : enabled : false postgresqlUsername : <chart name> postgresqlPassword : <chart password> postgresqlDatabase : <chart name> persistence : enabled : false","title":"Postgres"},{"location":"our-helm-charts/development/databases/#influxdb","text":"... influxdb : enabled : false architecture : standalone database : <chart name> authEnabled : false persistence : enabled : false","title":"InfluxDB"},{"location":"our-helm-charts/development/unit-tests/","text":"Unit tests \u00b6 We unit test our common library, while it isn't near complete coverage but it does offer some basic checks. Running these tests can be done any way you like. In this document we describe a number of approaches. Directly on your development machine \u00b6 First set up the environment: go mod download Run the tests: go test ./charts/.../tests Using Visual Studio Code \u00b6 Our repo comes with a Visual Studio Code development container definition and launch.json that allow you to quickly set up an environment in which you can run the tests. Prerequisites \u00b6 Visual Studio Code is installed. Docker is installed and running. The \"Remote - Containers\" extension is installed and enabled in Visual Studio Code. For more details, please refer to the official documentation . Running tests \u00b6 Once Visual Studio Code is set up, and you open the charts workspace, you will see a popup asking if you wish to re-open the workspace in a development container: Select the option that you prefer. The workspace will be reopened and a Dockerized workspace will be built. You can now use Visual Studio Code as normal. To run or debug the unit tests, click the \"Run\" button on the left sidebar and select the desired configuration: stable/common tests : This configuration will run the all test files for the common library chart. Next, press the green \"Play\" icon. This will start the tests show the outcome in a terminal window. Using a local Docker container \u00b6 The Visual Studio Code development container can also be leveraged without using Visual Studio Code. Prerequisites \u00b6 Docker is installed and running. You have the charts repo root folder opened in your shell of choice. The commands in this article assume you are running a Bash-compatible shell. Running tests \u00b6 The first step is to build the development container image containing the required tools. This step only needs to be done once. To build the container, run this command in your shell: docker build -t k8s-at-spiti/charts-unit-test -f .devcontainer/Dockerfile . When you wish to run the tests, run this command in your shell: docker run --rm -it -l \\ -v $( pwd ) :/charts --entrypoint \"/bin/bash\" \\ -w /charts k8s-at-spiti/charts-unit-test \\ -c \"go mod download && go test ./charts/.../tests\" This will create a container with the charts repo root folder mounted to /charts and execute all the test files belonging to the different charts. Note Depending on the performance of your environment, this can take a long time where it seems as if your machine is not doing anything! Output \u00b6 A successful test will output something like the following... ok github.com/k8s-at-spiti/library-charts/charts/stable/common/tests 54.087s","title":"Unit tests"},{"location":"our-helm-charts/development/unit-tests/#unit-tests","text":"We unit test our common library, while it isn't near complete coverage but it does offer some basic checks. Running these tests can be done any way you like. In this document we describe a number of approaches.","title":"Unit tests"},{"location":"our-helm-charts/development/unit-tests/#directly-on-your-development-machine","text":"First set up the environment: go mod download Run the tests: go test ./charts/.../tests","title":"Directly on your development machine"},{"location":"our-helm-charts/development/unit-tests/#using-visual-studio-code","text":"Our repo comes with a Visual Studio Code development container definition and launch.json that allow you to quickly set up an environment in which you can run the tests.","title":"Using Visual Studio Code"},{"location":"our-helm-charts/development/unit-tests/#prerequisites","text":"Visual Studio Code is installed. Docker is installed and running. The \"Remote - Containers\" extension is installed and enabled in Visual Studio Code. For more details, please refer to the official documentation .","title":"Prerequisites"},{"location":"our-helm-charts/development/unit-tests/#running-tests","text":"Once Visual Studio Code is set up, and you open the charts workspace, you will see a popup asking if you wish to re-open the workspace in a development container: Select the option that you prefer. The workspace will be reopened and a Dockerized workspace will be built. You can now use Visual Studio Code as normal. To run or debug the unit tests, click the \"Run\" button on the left sidebar and select the desired configuration: stable/common tests : This configuration will run the all test files for the common library chart. Next, press the green \"Play\" icon. This will start the tests show the outcome in a terminal window.","title":"Running tests"},{"location":"our-helm-charts/development/unit-tests/#using-a-local-docker-container","text":"The Visual Studio Code development container can also be leveraged without using Visual Studio Code.","title":"Using a local Docker container"},{"location":"our-helm-charts/development/unit-tests/#prerequisites_1","text":"Docker is installed and running. You have the charts repo root folder opened in your shell of choice. The commands in this article assume you are running a Bash-compatible shell.","title":"Prerequisites"},{"location":"our-helm-charts/development/unit-tests/#running-tests_1","text":"The first step is to build the development container image containing the required tools. This step only needs to be done once. To build the container, run this command in your shell: docker build -t k8s-at-spiti/charts-unit-test -f .devcontainer/Dockerfile . When you wish to run the tests, run this command in your shell: docker run --rm -it -l \\ -v $( pwd ) :/charts --entrypoint \"/bin/bash\" \\ -w /charts k8s-at-spiti/charts-unit-test \\ -c \"go mod download && go test ./charts/.../tests\" This will create a container with the charts repo root folder mounted to /charts and execute all the test files belonging to the different charts. Note Depending on the performance of your environment, this can take a long time where it seems as if your machine is not doing anything!","title":"Running tests"},{"location":"our-helm-charts/development/unit-tests/#output","text":"A successful test will output something like the following... ok github.com/k8s-at-spiti/library-charts/charts/stable/common/tests 54.087s","title":"Output"},{"location":"our-own-code-projects/introduction/","text":"Introduction \u00b6 When code for the containers is mainteined by the k8s-at-spiti community, we keep the projects in their own GitHub project instead of container images . This allows the project to cover the development and release of projects versions there. The projects build containers and upload them to GHCR so for the users they do not differ from other images. The development process is on the other hand different from the container images so please refer to the development docummentation for the details.","title":"Introduction"},{"location":"our-own-code-projects/introduction/#introduction","text":"When code for the containers is mainteined by the k8s-at-spiti community, we keep the projects in their own GitHub project instead of container images . This allows the project to cover the development and release of projects versions there. The projects build containers and upload them to GHCR so for the users they do not differ from other images. The development process is on the other hand different from the container images so please refer to the development docummentation for the details.","title":"Introduction"},{"location":"our-own-code-projects/development/creating-a-new-container-image/","text":"Creating a new container image \u00b6 Dependencies \u00b6 If you would like to help create new container images, there's a few tools you will need: make Docker buildx any other compiler tools used to build and test your code Creating a new project \u00b6 Create a new GitHub project using this template as reference. If you create this project under k8s-at-spiti then you are ready to start coding. If this is not the case you will need to create required GitHub secrets as described in the template README . The template includes are required workflows we use to build multi-arch containers. It also contains a sync workflow that will generate pull requests when the template .github folder changes. Adding your code \u00b6 The project contains an example Golang program and a Makefile to test it. If your project uses Golang you can then start extending it. If you use a different programming languaje you will need to remove the go files and adjust the Makefile. Building and testing \u00b6 make will build and test the code make docker-build will build the container Testing the CI pipeline in GitHub \u00b6 After you upload your changes to GitHub the CI workflow action should be invoqued automatically. If it runs sucesfully your container should not be uploaded to GHCR . It will be private so you will need to ask a community mainteiner to make it public. References \u00b6 Check the following examples of code projects:","title":"Creating a new project"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#creating-a-new-container-image","text":"","title":"Creating a new container image"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#dependencies","text":"If you would like to help create new container images, there's a few tools you will need: make Docker buildx any other compiler tools used to build and test your code","title":"Dependencies"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#creating-a-new-project","text":"Create a new GitHub project using this template as reference. If you create this project under k8s-at-spiti then you are ready to start coding. If this is not the case you will need to create required GitHub secrets as described in the template README . The template includes are required workflows we use to build multi-arch containers. It also contains a sync workflow that will generate pull requests when the template .github folder changes.","title":"Creating a new project"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#adding-your-code","text":"The project contains an example Golang program and a Makefile to test it. If your project uses Golang you can then start extending it. If you use a different programming languaje you will need to remove the go files and adjust the Makefile.","title":"Adding your code"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#building-and-testing","text":"make will build and test the code make docker-build will build the container","title":"Building and testing"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#testing-the-ci-pipeline-in-github","text":"After you upload your changes to GitHub the CI workflow action should be invoqued automatically. If it runs sucesfully your container should not be uploaded to GHCR . It will be private so you will need to ask a community mainteiner to make it public.","title":"Testing the CI pipeline in GitHub"},{"location":"our-own-code-projects/development/creating-a-new-container-image/#references","text":"Check the following examples of code projects:","title":"References"},{"location":"support/code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at Devin buhl devin.kray@gmail.com or Jeff Billimek jeff@billimek.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"support/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"support/code-of-conduct/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"support/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"support/code-of-conduct/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"support/code-of-conduct/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"support/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at Devin buhl devin.kray@gmail.com or Jeff Billimek jeff@billimek.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"support/code-of-conduct/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"support/code-of-conduct/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"support/code-of-conduct/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"support/code-of-conduct/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"support/code-of-conduct/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"support/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"support/community/","text":"Community \u00b6 We have a few outlets for getting community support with our projects: Issues \u00b6 If you encounter an issue when using our charts or container images, please add it to their respective GitHub Issues tracker. Please make sure you use the provided issue templates so that can better understand your issue as well as duplicate it on our end! Forum \u00b6 We use GitHub Discussions to provide a forum to hold discussions among the community. Everything that is not an issue but is related to k8s@spiti can be discussed here: use-cases, requests for help and suggestions, discussions on the future of projects, and other similar topics. Chat \u00b6 We use Discord as a chat solution for allowing both k8s@spiti users and developers to interact in real time. Click here for an invitation.","title":"Community"},{"location":"support/community/#community","text":"We have a few outlets for getting community support with our projects:","title":"Community"},{"location":"support/community/#issues","text":"If you encounter an issue when using our charts or container images, please add it to their respective GitHub Issues tracker. Please make sure you use the provided issue templates so that can better understand your issue as well as duplicate it on our end!","title":"Issues"},{"location":"support/community/#forum","text":"We use GitHub Discussions to provide a forum to hold discussions among the community. Everything that is not an issue but is related to k8s@spiti can be discussed here: use-cases, requests for help and suggestions, discussions on the future of projects, and other similar topics.","title":"Forum"},{"location":"support/community/#chat","text":"We use Discord as a chat solution for allowing both k8s@spiti users and developers to interact in real time. Click here for an invitation.","title":"Chat"},{"location":"support/faq/","text":"Frequently Asked Questions \u00b6 Why aren't the charts updated with each image release? \u00b6 k8s@spiti feels that it is the responsibility of the person deploying the chart to ensure that the most up to date image is being deployed as well as keeping the deployed images up to date. Keeping images would be a full time job due to the frequent code changes and so the the k8s@spiti community would rather spend their time implementing new features and charts for everyone to use.","title":"FAQ"},{"location":"support/faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"support/faq/#why-arent-the-charts-updated-with-each-image-release","text":"k8s@spiti feels that it is the responsibility of the person deploying the chart to ensure that the most up to date image is being deployed as well as keeping the deployed images up to date. Keeping images would be a full time job due to the frequent code changes and so the the k8s@spiti community would rather spend their time implementing new features and charts for everyone to use.","title":"Why aren't the charts updated with each image release?"}]}